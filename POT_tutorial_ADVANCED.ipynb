{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcOJcaApdauY"
   },
   "source": [
    "# Postraining Optimization Toolkit. Advanced topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYlPqVeXTPDw"
   },
   "source": [
    "This notebook considers how to support custom model quantization via POT with Accuracy Checker backend which is a fundamental part of POT responsible for data reading, pre- & post- processing, inference launching and metrics collection.\n",
    "\n",
    "Accuracy Checker is the tool for models accuracy validation. It has a lot of predefined configuration options for dataset conversion, preprocessing, postprocessing and metric evaluation. However sometimes its capabilities is not enough for running particular model.\n",
    "\n",
    "Lets duscuss, what should we do in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequsites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/opencv/open_model_zoo.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_PITDgseUtTx"
   },
   "source": [
    "## First of all, sa couple of words about Accuracy Checker architecture.\n",
    "\n",
    "Accuracy Checker has a  modular structure. This tool is very flexible and easy to extend with new components.\n",
    "\n",
    "The common aproach of adding new functionality includes the following steps:\n",
    "1. Create a new class for the component. \n",
    "2. The parent class of the new component should be basic abstract class for all objects of extended module.\n",
    "3. Implement all abstract methods for a base class in the new component.\n",
    "4. Define the name for configuration in the `__provider__` field.\n",
    "5. Optionally, if component should have configurable parameters, you need to specify them in `parameters()` method.\n",
    "6. Finally, you need to register new functionality inside __init__.py of extendable module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mudCcygoXX_Q"
   },
   "source": [
    "## Case 1: Emotion Recognition\n",
    "\n",
    "Now we'll go through all these steps on the example of [Emotion Recognition model](https://github.com/onnx/models/tree/master/vision/body_analysis/emotion_ferplus) from onnx model zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5wifB_gmYHHP"
   },
   "source": [
    "### Step 0 - Analyze the Model\n",
    "At the begining, we need to understand which component should we modify.\n",
    "It's not enough to just convert the model to IR for that reason - we need to get to know the model better. \n",
    "\n",
    "During the analysis you should find answers to the following questions:\n",
    "\n",
    "*   What is the model use case? For which purpose does it used?\n",
    "*   Which dataset should I use for evaluation?\n",
    "*   Which preprocessing steps should be preformed?\n",
    "*   How to retrive results from the model and postprocess them?\n",
    "*   Which metric should be used?\n",
    "\n",
    "Let's come back to our emotion recognition model. We can find all needed information from readme - https://github.com/onnx/models/blob/master/vision/body_analysis/emotion_ferplus/README.md\n",
    "\n",
    "*   What is the model use case? For which purpose does it used? - __emotions_recognition__, __classification__\n",
    "*   Which dataset should I use for evaluation? **FER+ stored in csv format** \n",
    "*   Which preprocessing steps should be preformed? **resize image with antialias interpolation to 64x64 size**\n",
    "*   How to retrive results from the model and postprocess them? **model returns tensor with shape [N, 8], where N - number images in batch size. It contains probabilities in raw logits format. Possibly softmax operation should be applied.** \n",
    "*   Which metric should be used? **emotion recognition is particular case of classification task, so classification accuracy can be suitable for model evaluation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6JqZv3_tFXQ"
   },
   "source": [
    "### Step 1. Support new dataset.\n",
    "\n",
    "Emotion recognition model was trained on FER+ dataset. It can be download from here - https://www.kaggle.com/deadskull7/fer2013\n",
    "\n",
    "The dataset represented as csv table with 3 fields:\n",
    "* *emotion* - ground truth label\n",
    "* *pixels* - array of pixel intensity which represent gray scale image in size 48x48. Also it make sence to say that they given in flatten format row by row.\n",
    "* *usage* - split of dataset which image belongs (`Training`, `PublicTest`). For validation and quantization we only interested in validation part, so we should implement image filtering by this field.\n",
    "\n",
    "Keeping in mind all these details, lets implement converter for dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4mvPn2tBnI6"
   },
   "outputs": [],
   "source": [
    "%%writefile advanced_materials/fer_plus_converter.py\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# classes which represent configuration parameters in the code\n",
    "from ..config import PathField, BoolField\n",
    "# data type which was generated during annotation conversion\n",
    "from ..representation import ClassificationAnnotation\n",
    "from ..utils import read_csv\n",
    "\n",
    "from .format_converter import BaseFormatConverter, ConverterReturn\n",
    "\n",
    "\n",
    "\n",
    "class FERPlusFormatConverter(BaseFormatConverter):\n",
    "    \"\"\"\n",
    "    FER+ dataset converter. All annotation converters should be derived from BaseFormatConverter class.\n",
    "    Annotation data for conversion can be found here https://www.kaggle.com/deadskull7/fer2013\n",
    "    \"\"\"\n",
    "\n",
    "    # register name for this converter\n",
    "    # this name will be used for converter class look up\n",
    "    __provider__ = 'fer_plus'\n",
    "    # specify a hint about generated data type\n",
    "    annotation_types = (ClassificationAnnotation, )\n",
    "\n",
    "    @classmethod\n",
    "    def parameters(cls):\n",
    "        \"\"\"\n",
    "        describe config parsing template for this converter\n",
    "        :return: dictionary, where config fields used as keys and helpers for config parsing as values.\n",
    "        \"\"\"\n",
    "        # get basic parameters from parent class\n",
    "        configuration_parameters = super().parameters()\n",
    "        # update them with new\n",
    "        configuration_parameters.update({\n",
    "            'annotation_file': PathField(description=\"Path to csv file which contain dataset.\"),\n",
    "            'convert_images': BoolField(\n",
    "                optional=True,\n",
    "                default=False,\n",
    "                description=\"Allows to convert images from pickle file to user specified directory.\"\n",
    "            ),\n",
    "            'converted_images_dir': PathField(\n",
    "                optional=True, is_directory=True, check_exists=False, description=\"Path to converted images location.\"\n",
    "            )\n",
    "        })\n",
    "\n",
    "        return configuration_parameters\n",
    "\n",
    "    def configure(self):\n",
    "        \"\"\"\n",
    "        This method is responsible for obtaining the necessary parameters\n",
    "        for converting from the command line or config.\n",
    "        \"\"\"\n",
    "        self.csv_file = self.get_value_from_config('annotation_file')\n",
    "        self.converted_images_dir = self.get_value_from_config('converted_images_dir')\n",
    "        self.convert_images = self.get_value_from_config('convert_images')\n",
    "        if self.convert_images and not self.converted_images_dir:\n",
    "            self.converted_images_dir = self.test_csv_file.parent / 'converted_images'\n",
    "            if not self.converted_images_dir.exists():\n",
    "                self.converted_images_dir.mkdir(parents=True)\n",
    "\n",
    "        if self.convert_images and Image is None:\n",
    "            raise ValueError(\n",
    "                \"conversion mnist images requires Pillow installation, please install it before usage\"\n",
    "            )\n",
    "\n",
    "    def convert(self, check_content=False, progress_callback=None, progress_interval=100, **kwargs):\n",
    "        \"\"\"\n",
    "        This method is executed automatically when convert.py is started.\n",
    "        All arguments are automatically got from command line arguments or config file in method configure\n",
    "\n",
    "        Returns:\n",
    "            annotations: list of annotation representation objects.\n",
    "            meta: dictionary with additional dataset level metadata.\n",
    "            content errors: service field for errors handling\n",
    "        \"\"\"\n",
    "        annotations = []\n",
    "        # read original dataset annotation\n",
    "        annotation_table = read_csv(self.csv_file)\n",
    "        # process object by object\n",
    "        for index, annotation in enumerate(annotation_table):\n",
    "            # ignore data not from testing subset\n",
    "            if annotation['Usage'] != 'PublicTest':\n",
    "                continue\n",
    "            # identifier is unique name of data in the dataset. For images usually file name used\n",
    "            identifier = '{}.png'.format(index)\n",
    "            # getting label\n",
    "            label = int(annotation['emotion'])\n",
    "            # since our annotation contains pixels intensity inside the table,\n",
    "            # we need to get images from it for more convenient usage.\n",
    "            # convert images once, we can turn off this flag in the config and use pregenerated images\n",
    "            if self.convert_images:\n",
    "                pixels_array = [int(y) for y in annotation['pixels'].split()]\n",
    "                pixels = np.array(pixels_array).reshape(48, 48)\n",
    "                image = Image.fromarray(pixels)\n",
    "                image = image.convert(\"L\")\n",
    "                image.save(str(self.converted_images_dir / identifier))\n",
    "            # create a new instance of the annotation representation\n",
    "            # different representations can have different set of parameters required for metric calculation\n",
    "            # for ClassificationAnnotation, identifier and label used.\n",
    "            annotations.append(ClassificationAnnotation(identifier, label))\n",
    "\n",
    "        # metadata contains specific info about dataset which can help during the evaluation\n",
    "        # (e.g. mapping of labels, has background label in the dataset or not)\n",
    "        # for some task where additional info is not required it can be left empty of None\n",
    "        meta = self.get_meta()\n",
    "\n",
    "        # finally, this method should return the named tuple with fields annotations, meta and content errors\n",
    "        return ConverterReturn(annotations, meta, None)\n",
    "\n",
    "\n",
    "    def get_meta(self):\n",
    "        # use original lables from dataset\n",
    "        emotion_table = {'neutral': 0, 'happiness': 1, 'surprise': 2, 'sadness': 3, 'anger': 4, 'disgust': 5, 'fear': 6,\n",
    "                         'contempt': 7}\n",
    "        # inside Accuracy Checker we use class_id as label, so label map should be represented as class_id: class_name\n",
    "        label_map = {v: k  for v, k in emotion_table.items()}\n",
    "\n",
    "        # dataset meta should be represented like dictionary, label_map key used for storing label mapping\n",
    "        return {'label_map': label_map}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMWH9wUzvWkE"
   },
   "source": [
    "Now, lets integrate it to Accuracy Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "01ckAKATvVv_"
   },
   "outputs": [],
   "source": [
    "!cp advanced_materials/fer_plus_converter.py open_model_zoo/tools/accuracy_checker/accuracy_checker/annotation_converters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4IG_2uhv8sv"
   },
   "source": [
    "New class registration involves import new functionality inside `__init__.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVXNK7hcvxsc"
   },
   "outputs": [],
   "source": [
    "%%writefile open_model_zoo/tools/accuracy_checker/accuracy_checker/annotation_converters/__init__.py\n",
    "\"\"\"\n",
    "Copyright (c) 2019 Intel Corporation\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "from .format_converter import BaseFormatConverter\n",
    "from .convert import make_subset, save_annotation, analyze_dataset\n",
    "from .market1501 import Market1501Converter\n",
    "from .mars import MARSConverter\n",
    "from .pascal_voc import PascalVOCDetectionConverter\n",
    "from .sample_converter import SampleConverter\n",
    "from .wider import WiderFormatConverter\n",
    "from .detection_opencv_storage import DetectionOpenCVStorageFormatConverter\n",
    "from .lfw import LFWConverter\n",
    "from .vgg_face_regression import VGGFaceRegressionConverter\n",
    "from .super_resolution_converter import SRConverter, SRMultiFrameConverter\n",
    "from .imagenet import ImageNetFormatConverter\n",
    "from .icdar import ICDAR13RecognitionDatasetConverter, ICDAR15DetectionDatasetConverter\n",
    "from .kondate_nakayosi import KondateNakayosiRecognitionDatasetConverter\n",
    "from .ms_coco import MSCocoDetectionConverter, MSCocoKeypointsConverter, MSCocoSingleKeypointsConverter\n",
    "from .cityscapes import CityscapesConverter\n",
    "from .ncf_converter import MovieLensConverter\n",
    "from .brats import BratsConverter, BratsNumpyConverter\n",
    "from .cifar import CifarFormatConverter\n",
    "from .mnist import MNISTCSVFormatConverter\n",
    "from .wmt import WMTConverter\n",
    "from .common_semantic_segmentation import CommonSegmentationConverter\n",
    "from .camvid import CamVidConverter\n",
    "from .lpr import LPRConverter\n",
    "from .image_retrieval import ImageRetrievalConverter\n",
    "from .cvat_object_detection import CVATObjectDetectionConverter\n",
    "from .cvat_attributes_recognition import CVATAttributesRecognitionConverter\n",
    "from .cvat_age_gender_recognition import CVATAgeGenderRecognitionConverter\n",
    "from .cvat_facial_landmarks import CVATFacialLandmarksRecognitionConverter\n",
    "from .cvat_text_recognition import CVATTextRecognitionConverter\n",
    "from .cvat_multilabel_recognition import CVATMultilabelAttributesRecognitionConverter\n",
    "from .cvat_human_pose import CVATPoseEstimationConverter\n",
    "from .cvat_person_detection_action_recognition import CVATPersonDetectionActionRecognitionConverter\n",
    "from .squad import SQUADConverter\n",
    "from .text_classification import (\n",
    "    XNLIDatasetConverter,\n",
    "    BertXNLITFRecordConverter,\n",
    "    IMDBConverter,\n",
    "    MRPCConverter,\n",
    "    CoLAConverter\n",
    ")\n",
    "from .cmu_panoptic import CmuPanopticKeypointsConverter\n",
    "from .action_recognition import ActionRecognitionConverter\n",
    "from .ms_asl_continuous import MSASLContiniousConverter\n",
    "\n",
    "from .monocular_depth_perception import ReDWebDatasetConverter\n",
    "\n",
    "from .fashion_mnist import FashionMnistConverter\n",
    "from .inpainting import InpaintingConverter\n",
    "from .fer_plus_converter import FERPlusFormatConverter\n",
    "\n",
    "__all__ = [\n",
    "    'BaseFormatConverter',\n",
    "    'make_subset',\n",
    "    'save_annotation',\n",
    "    'analyze_dataset',\n",
    "\n",
    "    'ImageNetFormatConverter',\n",
    "    'Market1501Converter',\n",
    "    'SampleConverter',\n",
    "    'PascalVOCDetectionConverter',\n",
    "    'WiderFormatConverter',\n",
    "    'MARSConverter',\n",
    "    'DetectionOpenCVStorageFormatConverter',\n",
    "    'LFWConverter',\n",
    "    'VGGFaceRegressionConverter',\n",
    "    'SRConverter',\n",
    "    'SRMultiFrameConverter',\n",
    "    'ICDAR13RecognitionDatasetConverter',\n",
    "    'ICDAR15DetectionDatasetConverter',\n",
    "    'KondateNakayosiRecognitionDatasetConverter',\n",
    "    'MSCocoKeypointsConverter',\n",
    "    'MSCocoSingleKeypointsConverter',\n",
    "    'MSCocoDetectionConverter',\n",
    "    'CityscapesConverter',\n",
    "    'MovieLensConverter',\n",
    "    'BratsConverter',\n",
    "    'BratsNumpyConverter',\n",
    "    'CifarFormatConverter',\n",
    "    'MNISTCSVFormatConverter',\n",
    "    'WMTConverter',\n",
    "    'CommonSegmentationConverter',\n",
    "    'CamVidConverter',\n",
    "    'LPRConverter',\n",
    "    'ImageRetrievalConverter',\n",
    "    'CVATObjectDetectionConverter',\n",
    "    'CVATAttributesRecognitionConverter',\n",
    "    'CVATAgeGenderRecognitionConverter',\n",
    "    'CVATFacialLandmarksRecognitionConverter',\n",
    "    'CVATTextRecognitionConverter',\n",
    "    'CVATMultilabelAttributesRecognitionConverter',\n",
    "    'CVATPoseEstimationConverter',\n",
    "    'CVATPersonDetectionActionRecognitionConverter',\n",
    "    'SQUADConverter',\n",
    "    'XNLIDatasetConverter',\n",
    "    'BertXNLITFRecordConverter',\n",
    "    'IMDBConverter',\n",
    "    'MRPCConverter',\n",
    "    'CoLAConverter',\n",
    "    'CmuPanopticKeypointsConverter',\n",
    "    'ActionRecognitionConverter',\n",
    "    'MSASLContiniousConverter',\n",
    "    'ReDWebDatasetConverter',\n",
    "    'FashionMnistConverter',\n",
    "    'InpaintingConverter',\n",
    "    'FERPlusFormatConverter'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItPrGjcfxQ0C"
   },
   "source": [
    "### Step 2. Implement the preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALXzoFuRxW0S"
   },
   "source": [
    "Adding new preprocessor looks simmilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8evagMZQv5dV"
   },
   "outputs": [],
   "source": [
    "# %load advanced_materials/emotion_recognition_preprocessing.py\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# base class for all preprocessors is Preprocessor\n",
    "from ..preprocessor import Preprocessor\n",
    "# helpers for configuration parsing\n",
    "from ..config import NumberField\n",
    "\n",
    "\n",
    "class EmotionRecognitionResize(Preprocessor):\n",
    "    # name of preprocessor for configuration\n",
    "    __provider__ = 'emotion_recognition_preprocessing'\n",
    "\n",
    "    # definition of important configuration parameters\n",
    "    # for image resizing we need to know target size\n",
    "    @classmethod\n",
    "    def parameters(cls):\n",
    "        parameters = super().parameters()\n",
    "        parameters.update({\n",
    "            'size': NumberField(\n",
    "                value_type=int, optional=False, min_value=1, description=\"Destination sizes for both dimensions.\"\n",
    "            ),\n",
    "        })\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    def configure(self):\n",
    "        # getting parameters from config\n",
    "        self.size = self.get_value_from_config('size')\n",
    "\n",
    "    def process(self, image, annotation_meta=None):\n",
    "        \"\"\"\n",
    "        Preprocessor realization function, which will be called for each image in the input dataset\n",
    "        :param image: DataRepresentation entry which include read image and related metadata for it.\n",
    "        :param annotation_meta: Dictionary with info from  annotation.\n",
    "                                optional, used in case when we need to use or update some info about image\n",
    "        :return: DataRepresentation with updated image\n",
    "        \"\"\"\n",
    "        # image dasta stored inside DataRepresentation in data field\n",
    "        data = image.data\n",
    "        # internally we work with numpy arrays, so we need to convert it to pillow image object for resizing\n",
    "        resized_data = Image.fromarray(data).resize((self.size, self.size), Image.ANTIALIAS)\n",
    "        # return back data to numpy array\n",
    "        data = np.array(resized_data)\n",
    "        # expand dims for gray scale image\n",
    "        if len(data.shape) == 2:\n",
    "            data = np.expand_dims(data, axis=-1)\n",
    "        image.data = data\n",
    "        # returns updated DataRepresentation\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O-oBror5yZv0"
   },
   "outputs": [],
   "source": [
    "!cp advanced_materials/emotion_recognition_preprocessing.py open_model_zoo/tools/accuracy_checker/accuracy_checker/preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "78jPyETzyRhN"
   },
   "outputs": [],
   "source": [
    "%%writefile open_model_zoo/tools/accuracy_checker/accuracy_checker/preprocessor/__init__.py\n",
    "\"\"\"\n",
    "Copyright (c) 2019 Intel Corporation\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "from .preprocessing_executor import PreprocessingExecutor\n",
    "from .preprocessor import Preprocessor\n",
    "from .color_space_conversion import BgrToRgb, RgbToBgr, BgrToGray, RgbToGray, TfConvertImageDType, SelectInputChannel\n",
    "from .normalization import Normalize, Normalize3d\n",
    "from .geometric_transformations import (\n",
    "    GeometricOperationMetadata,\n",
    "    Flip,\n",
    "    Crop,\n",
    "    CropRect,\n",
    "    ExtendAroundRect,\n",
    "    PointAligner,\n",
    "    Tiling,\n",
    "    Crop3D,\n",
    "    TransformedCropWithAutoScale,\n",
    "    ImagePyramid\n",
    ")\n",
    "from .resize import Resize, AutoResize\n",
    "from .nlp_preprocessors import DecodeByVocabulary, PadWithEOS\n",
    "from .centernet_preprocessing import CenterNetAffineTransformation\n",
    "from .brats_preprocessing import Resize3D, NormalizeBrats, CropBraTS, SwapModalitiesBrats\n",
    "from .inpainting_preprocessor import FreeFormMask, RectMask, CustomMask\n",
    "from .emotion_recognition_preprocessing import EmotionRecognitionResize\n",
    "\n",
    "__all__ = [\n",
    "    'PreprocessingExecutor',\n",
    "\n",
    "    'Preprocessor',\n",
    "    'GeometricOperationMetadata',\n",
    "\n",
    "    'Resize',\n",
    "    'Resize3D',\n",
    "    'AutoResize',\n",
    "    'Flip',\n",
    "    'Crop',\n",
    "    'CropRect',\n",
    "    'ExtendAroundRect',\n",
    "    'PointAligner',\n",
    "    'Tiling',\n",
    "    'Crop3D',\n",
    "    'CropBraTS',\n",
    "    'TransformedCropWithAutoScale',\n",
    "    'ImagePyramid',\n",
    "\n",
    "    'BgrToGray',\n",
    "    'BgrToRgb',\n",
    "    'RgbToGray',\n",
    "    'RgbToBgr',\n",
    "    'TfConvertImageDType',\n",
    "    'SelectInputChannel',\n",
    "\n",
    "    'Normalize3d',\n",
    "    'Normalize',\n",
    "    'NormalizeBrats',\n",
    "\n",
    "    'SwapModalitiesBrats',\n",
    "\n",
    "    'DecodeByVocabulary',\n",
    "    'PadWithEOS',\n",
    "\n",
    "    'CenterNetAffineTransformation',\n",
    "\n",
    "    'FreeFormMask',\n",
    "    'RectMask',\n",
    "    'CustomMask',\n",
    "    \n",
    "    'EmotionRecognitionResize'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Twh4CxaEyyq-"
   },
   "source": [
    "### Step 3: implement output parsing and postprocessing.\n",
    "Adapter responsibility is to convert raw model output to  the suitable for metric calculation format. Inside accuracy checker, there is also postprocessor entity. The main difference between adapter and postprocessor is that postprocessor is an optional step in predicted data preparation - filtering, NMS, casting to integer, clipping and so on. Also postprocessor can work with annotation content in some cases (e.g. in popular datasets for semantic segmentation task, annotation represented as png mask where each class represented by specific color, for metric evaluation, we need to convert color to class ids).\n",
    "\n",
    "In our task, we do not need additional postprocessing steps, so an adapter will be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTak0h5g1G0g"
   },
   "outputs": [],
   "source": [
    "!cp advanced_materials/emotion_recognition_adapter.py open_model_zoo/tools/accuracy_checker/accuracy_checker/adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAMZtKd20r0l"
   },
   "outputs": [],
   "source": [
    "# %load advanced_materials/emotion_recognition_adapter.py\n",
    "# base class for all adapters\n",
    "from ..adapters import Adapter\n",
    "# output format\n",
    "from ..representation import ClassificationPrediction\n",
    "\n",
    "\n",
    "class EmotionRecognitionAdapter(Adapter):\n",
    "    \"\"\"\n",
    "    Class for converting output of emotion recognition model to ClassificationPrediction representation\n",
    "    \"\"\"\n",
    "    # new adapter name in the config\n",
    "    __provider__ = 'emotion_recognition'\n",
    "    # like other components adapter might have parameters for configuration, but in our case they are not used\n",
    "    # we can use default implementation of these parameters\n",
    "\n",
    "    def process(self, raw, identifiers=None, frame_meta=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            identifiers: list of input data identifiers\n",
    "            raw: output of model\n",
    "            frame_meta: list of meta information about each frame\n",
    "        Returns:\n",
    "            list of ClassificationPrediction objects\n",
    "        \"\"\"\n",
    "        # in some cases output can be returned as a list of dictionaries, while dictionary is expected.\n",
    "        # We need handle it inside extract prediction\n",
    "        prediction = self._extract_predictions(raw, frame_meta)[self.output_blob]\n",
    "\n",
    "        # define container to store a batch of predictions as independent entities\n",
    "        result = []\n",
    "        # go through batch dementions and extract results for specific image\n",
    "        for identifier, output in zip(identifiers, prediction):\n",
    "            # depending on the task output representation can be different and has it's own parameters\n",
    "            # for classification, identifier and class probabilities are required\n",
    "            single_prediction = ClassificationPrediction(identifier, output)\n",
    "            result.append(single_prediction)\n",
    "\n",
    "        # return list of prediction representations\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8EgDaE0yxtH"
   },
   "outputs": [],
   "source": [
    "%%writefile open_model_zoo/tools/accuracy_checker/accuracy_checker/adapters/__init__.py\n",
    "\"\"\"\n",
    "Copyright (c) 2019 Intel Corporation\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "from .adapter import Adapter, AdapterField, create_adapter\n",
    "\n",
    "from .action_recognition import ActionDetection\n",
    "from .text_detection import (\n",
    "    TextDetectionAdapter,\n",
    "    TextProposalsDetectionAdapter,\n",
    "    EASTTextDetectionAdapter\n",
    ")\n",
    "\n",
    "from .text_recognition import (\n",
    "    BeamSearchDecoder,\n",
    "    CTCGreedySearchDecoder,\n",
    "    LPRAdapter\n",
    ")\n",
    "\n",
    "from .image_processing import SuperResolutionAdapter\n",
    "from .attributes_recognition import (\n",
    "    HeadPoseEstimatorAdapter,\n",
    "    VehicleAttributesRecognitionAdapter,\n",
    "    PersonAttributesAdapter,\n",
    "    AgeGenderAdapter,\n",
    "    LandmarksRegressionAdapter,\n",
    "    GazeEstimationAdapter\n",
    ")\n",
    "\n",
    "from .reidentification import ReidAdapter\n",
    "from .detection import (\n",
    "    SSDAdapter,\n",
    "    FacePersonAdapter,\n",
    "    TFObjectDetectionAPIAdapter,\n",
    "    SSDAdapterMxNet,\n",
    "    PyTorchSSDDecoder,\n",
    "    SSDONNXAdapter,\n",
    "    MTCNNPAdapter,\n",
    "    RetinaNetAdapter,\n",
    "    ClassAgnosticDetectionAdapter\n",
    ")\n",
    "from .yolo import TinyYOLOv1Adapter, YoloV2Adapter, YoloV3Adapter\n",
    "from .classification import ClassificationAdapter\n",
    "from .segmentation import SegmentationAdapter, BrainTumorSegmentationAdapter\n",
    "from .pose_estimation import HumanPoseAdapter\n",
    "from .pose_estimation_3d import HumanPose3dAdapter\n",
    "\n",
    "from .dummy_adapters import XML2DetectionAdapter\n",
    "\n",
    "from .hit_ratio import HitRatioAdapter\n",
    "\n",
    "from .mask_rcnn import MaskRCNNAdapter\n",
    "from .mask_rcnn_with_text import MaskRCNNWithTextAdapter\n",
    "\n",
    "from .nlp import MachineTranslationAdapter, QuestionAnsweringAdapter\n",
    "\n",
    "from .centernet import CTDETAdapter\n",
    "\n",
    "from .mono_depth import MonoDepthAdapter\n",
    "\n",
    "from .image_inpainting import ImageInpaintingAdapter\n",
    "from .emotion_recognition_adapter import EmotionRecognitionAdapter\n",
    "\n",
    "__all__ = [\n",
    "    'Adapter',\n",
    "    'AdapterField',\n",
    "    'create_adapter',\n",
    "\n",
    "    'XML2DetectionAdapter',\n",
    "\n",
    "    'ClassificationAdapter',\n",
    "\n",
    "    'SSDAdapter',\n",
    "    'FacePersonAdapter',\n",
    "    'TFObjectDetectionAPIAdapter',\n",
    "    'SSDAdapterMxNet',\n",
    "    'SSDONNXAdapter',\n",
    "    'PyTorchSSDDecoder',\n",
    "    'MTCNNPAdapter',\n",
    "    'CTDETAdapter',\n",
    "    'RetinaNetAdapter',\n",
    "    'ClassAgnosticDetectionAdapter',\n",
    "\n",
    "    'SegmentationAdapter',\n",
    "    'BrainTumorSegmentationAdapter',\n",
    "\n",
    "    'ReidAdapter',\n",
    "\n",
    "    'SuperResolutionAdapter',\n",
    "\n",
    "    'HeadPoseEstimatorAdapter',\n",
    "    'VehicleAttributesRecognitionAdapter',\n",
    "    'PersonAttributesAdapter',\n",
    "    'AgeGenderAdapter',\n",
    "    'LandmarksRegressionAdapter',\n",
    "    'GazeEstimationAdapter',\n",
    "\n",
    "    'TextDetectionAdapter',\n",
    "    'TextProposalsDetectionAdapter',\n",
    "    'EASTTextDetectionAdapter',\n",
    "\n",
    "    'BeamSearchDecoder',\n",
    "    'LPRAdapter',\n",
    "    'CTCGreedySearchDecoder',\n",
    "\n",
    "    'HumanPoseAdapter',\n",
    "    'HumanPose3dAdapter',\n",
    "\n",
    "    'ActionDetection',\n",
    "\n",
    "    'HitRatioAdapter',\n",
    "\n",
    "    'MaskRCNNAdapter',\n",
    "    'MaskRCNNWithTextAdapter',\n",
    "\n",
    "    'MachineTranslationAdapter',\n",
    "    'QuestionAnsweringAdapter',\n",
    "\n",
    "    'MonoDepthAdapter',\n",
    "\n",
    "    'ImageInpaintingAdapter',\n",
    "    \n",
    "    'EmotionRecognitionAdapter'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9GZQV6R1VYi"
   },
   "source": [
    "### Step 4: reinstall Accuracy Checker to apply new changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QR5MJIZgyqyV"
   },
   "outputs": [],
   "source": [
    "!pip3 install open_model_zoo/tools/accuracy_checker --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=\"$PWD/open_model_zoo/tools/accuracy_checker:$PYTHONPATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBIxOtj71wHm"
   },
   "source": [
    "### Step 5. Creating POT configuration file and running the quantization.\n",
    "\n",
    "Now, lets use introduced functionality inside POT config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2xnaJLkeDTt"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/onnx/models/raw/master/vision/body_analysis/emotion_ferplus/model/emotion-ferplus-8.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33QuuYAROUcx"
   },
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model emotion-ferplus-8.onnx --input_shape \"[1, 1, 64, 64]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-15_NbmeWqJ"
   },
   "outputs": [],
   "source": [
    "cat advanced_materials/emotion_recognition.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FF9WWxHa2k_G"
   },
   "outputs": [],
   "source": [
    "!pot -c emotion_recognition.json -e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBJr4__q2vOA"
   },
   "source": [
    "It works! Now we can modify `compression` section content to find quantized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7RPUoYg03M2j"
   },
   "source": [
    "## Case 2: Yolo V3\n",
    "\n",
    "Yolo V3 is complicated from the configuration perspective:\n",
    "1. labeles start with 0, while in majority of detection models uses dataset with background\n",
    "2. Model evaluation requires to know some details about output layers (anchors, cell size, e.t.c.)\n",
    "3. Model has several outputs\n",
    "\n",
    "Information about all necessary parameters can be found in Accuracy Checker readme.\n",
    "Beside that, there is also predefined config for model inside OMZ. It can be used as template for your own model evaluation.\n",
    "AccuracyChecker config for yolo_v3 can be found [here](https://github.com/opencv/open_model_zoo/blob/master/tools/accuracy_checker/configs/yolo-v3-tf.yml)\n",
    "\n",
    "So you do not need to modify Accuracy Checker for getting correct configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdMlD6iyPWPx"
   },
   "source": [
    "## Case 3: DCSCN\n",
    "\n",
    "Superresolution network with 5D input tensor, which represents sequence of 5 frames.\n",
    "\n",
    "At the first glance, it looks like as an unusual case where we need to create new entities for AC, but inside [annotation conversion guide](https://github.com/opencv/open_model_zoo/blob/master/tools/accuracy_checker/accuracy_checker/annotation_converters/README.md) we can find the following converter description:\n",
    "\n",
    "`multi_frame_super_resolution` - converts dataset for super resolution task with multiple input frames usage.\n",
    "* `data_dir` - path to folder, where images in low and high resolution are located.\n",
    "* `lr_suffix` - low resolution file name's suffix (default lr).\n",
    "* `hr_suffix` - high resolution file name's suffix (default hr).\n",
    "* `annotation_loader` - the library which will be used for ground truth image reading. Supported: opencv, pillow (Optional. Default value is pillow). Note, color space of an image depends on the loader (OpenCV uses BGR, Pillow uses RGB for image reading).\n",
    "* `number_input_frames` - the number of input frames per inference.\n",
    "\n",
    "It sound very simmilar on our use case, does not it?\n",
    "It can be adopted for our use case:\n",
    "* handle more complicated image names\n",
    "* use midle frame as gt reference\n",
    "\n",
    "More details provided in OMZ [pull request 1083](https://github.com/opencv/open_model_zoo/pull/1083)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "POT_training_ADVANCED",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
