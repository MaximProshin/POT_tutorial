{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFfNpMd0Q_1Q"
   },
   "source": [
    "# Post-training Optimization Toolkit Basics Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q8AW8FmpQ_1R"
   },
   "source": [
    "Post-training optimization Toolkit (POT) is helping to optimize the model applying different optimization techniques like low precision quantization and sparsity. Please refer to the documentation here: https://docs.openvinotoolkit.org/latest/_README.html In that tutorial we'll concentrate on INT8 quantization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates basic capabilities of POT:\n",
    "* POT configuration files structure\n",
    "* How to run POT in simplified mode\n",
    "* How to measure accuracy of FP32, INT8 models using POT config \n",
    "* How to create your own POT config\n",
    "* How to properly benchmark the workload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pp98oZdvQ_1T"
   },
   "source": [
    "## Step 0. Prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fnIxpsAlQ_1T"
   },
   "source": [
    "To do a quantization you need pre-trained model in IR format and calibration dataset. Let's prepare both components in this step. In this tutorial, we'll use SimpLeNet - very simple model trained specially for sample purposes on cifar-10 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBE3uq-uQ_1U"
   },
   "source": [
    "### Step 0.1. Converting model to IR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SampLeNet is distributed as a part of OpenVINO and used in AccuracyChecker sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch_model\t       SampLeNet.caffemodel  samplenet-symbol.json\n",
      "samplenet-0000.params  samplenet.onnx\t     SampLeNet.xml\n",
      "SampLeNet.bin\t       samplenet.pb\n",
      "SampLeNet.blob\t       SampLeNet.prototxt\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/open_model_zoo/tools/accuracy_checker/data/test_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RpOyw9dZQ_1U"
   },
   "source": [
    "Here are SampleNets trained with different frameworks. Let's take Caffe one and convert it to IR.\n",
    "BKM: To achieve proper accuracy always check what normalization was applied on the model training stage. Model Optimizer can apply mean and scale values if appropriate. This information can be obtained from the model training script.\n",
    "In our case means and scales were applied to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SmKq9KKuQ_1V",
    "outputId": "02836829-9758-495c-8481-99923d46676c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/opt/intel/openvino/deployment_tools/open_model_zoo/tools/accuracy_checker/data/test_models/SampLeNet.caffemodel\n",
      "\t- Path for generated IR: \t/home/u44153/My-Notebooks/POT_tutorial/IR\n",
      "\t- IR output name: \tSampLeNet\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \t[125.307,122.961,113.8575]\n",
      "\t- Scale values: \t[51.5865,50.847,51.255]\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Path to Python Caffe* parser generated from caffe.proto: \t/opt/intel/openvino/deployment_tools/model_optimizer/mo/front/caffe/proto\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/opt/intel/openvino/deployment_tools/open_model_zoo/tools/accuracy_checker/data/test_models/SampLeNet.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \tDefault\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "Model Optimizer version: \t\n",
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tprotobuf: installed: 3.12.2, required: == 3.6.1\n",
      "\n",
      "Please install required versions of components or use install_prerequisites script\n",
      "/opt/intel/openvino_2020.3.194/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites_caffe.sh\n",
      "Note that install_prerequisites scripts may install additional components.\n",
      "\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/u44153/My-Notebooks/POT_tutorial/IR/SampLeNet.xml\n",
      "[ SUCCESS ] BIN file: /home/u44153/My-Notebooks/POT_tutorial/IR/SampLeNet.bin\n",
      "[ SUCCESS ] Total execution time: 1.56 seconds. \n",
      "[ SUCCESS ] Memory consumed: 92 MB. \n"
     ]
    }
   ],
   "source": [
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model /opt/intel/openvino/deployment_tools/open_model_zoo/tools/accuracy_checker/data/test_models/SampLeNet.caffemodel \\\n",
    "--output_dir IR \\\n",
    "--mean_values [125.307,122.961,113.8575] \\\n",
    "--scale_values [51.5865,50.847,51.255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6t8hsPLBQ_1Y"
   },
   "source": [
    "### Step 0.2. Getting the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAgbF_nyQ_1Y"
   },
   "source": [
    "The dataset is an essential part of quantization. It's needed to collect calibration statistics and measure accuracy using accuracy checker tool. That's why big portion of information we'll need further is about the dataset on which model was trained.\n",
    "Let's download and prepare that. \n",
    "In our example the model was trained on cifar10 which consists of 60000 32x32 colour images in 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1VtTrsFQ_1Z",
    "outputId": "de0121dc-0fe1-4c1f-8c0b-41eb36f34e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-22 02:35:38--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
      "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 170498071 (163M) [application/x-gzip]\n",
      "Saving to: ‘cifar-10-python.tar.gz.1’\n",
      "\n",
      "cifar-10-python.tar 100%[===================>] 162.60M  13.2MB/s    in 17s     \n",
      "\n",
      "2020-06-22 02:35:55 (9.60 MB/s) - ‘cifar-10-python.tar.gz.1’ saved [170498071/170498071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a3HdKsbuQ_1c"
   },
   "source": [
    "Unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6GobgK1_Q_1c"
   },
   "outputs": [],
   "source": [
    "!tar -xzf cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZ-xS57fQ_1f",
    "outputId": "e3be63af-a4b9-44b9-cc99-570be020b882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta  data_batch_2  data_batch_4  readme.html\n",
      "data_batch_1  data_batch_3  data_batch_5  test_batch\n"
     ]
    }
   ],
   "source": [
    "!ls cifar-10-batches-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAhdRZGcQ_1j"
   },
   "source": [
    "## Step 1. Getting familiar with POT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iJVXu5DQ_1k"
   },
   "source": [
    "Let's check how POT is working and how to work with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0jlcpmRQ_1k",
    "outputId": "76409868-1375-4968-87be-98e2e84da50e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: pot [-h] -c CONFIG [-e] [--output-dir OUTPUT_DIR] [-d]\n",
      "           [--log-level {CRITICAL,ERROR,WARNING,INFO,DEBUG}] [--progress-bar]\n",
      "           [--keep_uncompressed_weights]\n",
      "\n",
      "Post-training Compression Toolkit\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        Path to a config file with task/model-specific\n",
      "                        parameters\n",
      "  -e, --evaluate        Whether to evaluate model on whole dataset\n",
      "  --output-dir OUTPUT_DIR\n",
      "                        The directory where models are saved. Default:\n",
      "                        ./results\n",
      "  -d, --direct-dump     Flag to save files without sub folders with algo names\n",
      "  --log-level {CRITICAL,ERROR,WARNING,INFO,DEBUG}\n",
      "                        Log level to print\n",
      "  --progress-bar        Disable CL logging and enable progress bar\n",
      "  --keep_uncompressed_weights\n",
      "                        Keep Convolution, Deconvolution and FullyConnected\n",
      "                        weights uncompressed\n"
     ]
    }
   ],
   "source": [
    "!pot -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uO85CCALQ_1n"
   },
   "source": [
    "As you can see, all \"magic\" is inside the config.json file, let's look into that. \n",
    "OpenVINO has POT config templates and config examples inside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WgQP0GK_Q_1o",
    "outputId": "4cbfac5a-31ac-4782-d7ec-100dee139d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calibration_dataset_template.json  template_accuracy_aware_quantization.json\n",
      "examples\t\t\t   template_default_quantization.json\n",
      "README.md\t\t\t   template_tpe.json\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PHZFdidQ_1q"
   },
   "source": [
    "Templates (template_accuracy_aware_quantization.json, template_default_quantization.json, template_tpe.json) contain all possible POT parameters with very detailed explanation. If you need no know meaning of certain parameter, this is a good resource to learn. \n",
    "Examples provide accuracy_checker, quantization and sparsity configs for several well-known public topologies. This is good material to getting started if you need to quantize models listed here or similar models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GC4Eh28pQ_1s",
    "outputId": "2f4008e9-ba0c-4f66-9b8f-a08601205033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_checker  quantization\tREADME.md  sparsity\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "srEL246dQ_1u",
    "outputId": "b3790c6b-1b82-411f-cc4f-c8c15626eac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densnet_121.yaml\t\t     mobilenet_v2.yaml\n",
      "east_resnet_v1_50.yaml\t\t     ncf.yaml\n",
      "faster_rcnn_resnet101_coco.yaml      ssd_mobilenet_v1.yaml\n",
      "faster_rcnn_resnet50_coco.yaml\t     ssd_resnet34.yaml\n",
      "mask_rcnn_resnet50_atrous_coco.yaml  ssd_resnet50_512.yml\n",
      "mobilenet_v1_tf.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/examples/accuracy_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vO3mxrpRQ_1w",
    "outputId": "42398992-1e78-47de-c972-343f59f75c9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification\t  optimization\t  segmentation\n",
      "object_detection  recommendation  text_detection\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/examples/quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjykMtPpQ_1z",
    "outputId": "b7f0693f-a62d-447c-e12d-95287424e376"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densnet_121_caffe_int8.json\t\t mobilenetV2_tf_int8.json\n",
      "inceptionV3_tf_int8.json\t\t mobilenetV2_tf_int8_simple_mode.json\n",
      "mobilenetV1_tf_int8.json\t\t mobilenetV2_tf_int8_sqnr.json\n",
      "mobilenetV2_engine_config.json\t\t se_resnet50_pytorch_int8.json\n",
      "mobilenetV2_pytorch_int8.json\t\t squeezenet1_1_pytorch_int8.json\n",
      "mobilenetV2_tf_int8_accuracy_aware.json\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/examples/quantization/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KaeOk-A8Q_11",
    "outputId": "8bfdc711-e3c8-45e8-eef4-35f17173b54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet-50-pytorch.json\tssd_resnet50_512.json\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/examples/sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nP2jUg1bQ_13"
   },
   "source": [
    "Let's take a look into the DefaultQuantization template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kKFb4qVTQ_14",
    "outputId": "f66dd360-786f-4c9e-e72d-e3a727a62a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    /* Model */\n",
      "    \"model\": {\n",
      "        \"model_name\": \"model_name\", // Model name\n",
      "        \"model\": \"<MODEL_PATH>\", // Path to model (.xml format)\n",
      "        \"weights\": \"<PATH_TO_WEIGHTS>\" // Path to weights (.bin format)\n",
      "    },\n",
      "    /* Parameters of the engine used for model inference. */\n",
      "    /* Post-Training Optimization Tool supports engine based on accuracy checker and custom engine.\n",
      "       For custom engine you should specify your own set of parameters.\n",
      "       The engine based on accuracy checker uses accuracy checker parameters. You can specify the parameters\n",
      "       via accuracy checker config file or directly in engine section.\n",
      "       More information about accuracy checker parameters can be found here:\n",
      "       https://github.com/opencv/open_model_zoo/tree/master/tools/accuracy_checker */\n",
      "    \"engine\": {\n",
      "        // Accuracy checker mode (default)\n",
      "        \"stat_requests_number\": 8,  // Number of requests during statistcs collection\n",
      "        \"eval_requests_number\": 8,  // Number of requests during evaluation\n",
      "        \"config\": \"<CONFIG_PATH>\",\n",
      "        /* OR */\n",
      "        \"name\": \"model_name\",\n",
      "        \"launchers\": [\n",
      "            {\n",
      "                \"framework\": \"dlsdk\",\n",
      "                \"device\": \"CPU\",\n",
      "                \"adapter\": \"classification\"\n",
      "            }\n",
      "        ],\n",
      "        \"datasets\": [\n",
      "            {\n",
      "                \"name\": \"dataset_name\",\n",
      "                \"data_source\": \"<DATASET_PATH>\",\n",
      "                \"annotation\": \"<ANNOTATION_PATH>\",\n",
      "                \"preprocessing\": [\n",
      "                    {\n",
      "                        \"type\": \"resize\",\n",
      "                        \"interpolation\": \"BILINEAR\",\n",
      "                        \"aspect_ratio_scale\": \"greater\",\n",
      "                        \"size\": 224\n",
      "                    }\n",
      "                ],\n",
      "                \"metrics\": [\n",
      "                    {\n",
      "                        \"name\": \"accuracy@top1\",\n",
      "                        \"type\": \"accuracy\",\n",
      "                        \"top_k\": 1\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        /* OR simplified mode */\n",
      "        \"type\": \"simplified\", // OR default value \"type\": \"accuracy_checker\" for non simplified mode\n",
      "        // you can specify path to directory with images\n",
      "        // also you can specify template for file names to filter images to load\n",
      "        // templates are unix style (This option valid only in simplified mode)\n",
      "        \"data_source\": \"PATH_TO_SOURCE\",\n",
      "    },\n",
      "    /* Optimization hyperparameters */\n",
      "    \"compression\": {\n",
      "        \"target_device\": \"CPU\", // target device, the specificity of which will be taken into account during optimization\n",
      "        \"algorithms\": [\n",
      "            {\n",
      "                \"name\": \"DefaultQuantization\", // optimization algorithm name\n",
      "                \"params\": {\n",
      "                    \"ignored\": {\n",
      "                        \"scope\": [\n",
      "                            \"<NODE_NAME>\" // List of nodes that are excluded from optimization\n",
      "                        ],\n",
      "                        \"operations\": [   // List of types that are excluded from optimization\n",
      "                            {\n",
      "                                \"type\": \"<NODE_TYPE>\",\n",
      "                                \"attributes\": {         // Includes excluding by attributes\n",
      "                                    \"<NAME>\": \"<VALUE>\" // Lists of values is not included\n",
      "                                }\n",
      "                            },\n",
      "                            {\n",
      "                                \"type\": \"<NODE_TYPE>\" // Excluding only by type\n",
      "                            }\n",
      "                        ]\n",
      "                    },\n",
      "                    /* A preset is a collection of optimization algorithm parameters that will specify to the algorithm\n",
      "                    to improve which metric the algorithm needs to concentrate. Each optimization algorithm supports\n",
      "                    [performance, accuracy] presets */\n",
      "                    \"preset\": \"accuracy\",\n",
      "                    \"stat_subset_size\": 100, // Size of subset to calculate activations statistics that can be used\n",
      "                                             // for quantization parameters calculation.\n",
      "                    /* Manually specification quantization parametrs */\n",
      "                    /* Quantization parameters for weights */\n",
      "\n",
      "                    \"weights\": {\n",
      "                        \"bits\": 8,           // Number of quantization bits\n",
      "                        \"mode\": \"symmetric\", // Quantization mode\n",
      "                        \"granularity\": \"perchannel\", // Granularity: a scale for each output channel.\n",
      "                        \"level_low\": -127, // Low quantization level\n",
      "                        \"level_high\": 127, // High quantization level\n",
      "                        /* Parameters specify how to calculate the minimum and maximum of quantization range */\n",
      "                        \"range_estimator\": {\n",
      "                            \"max\": {\n",
      "                                \"type\": \"quantile\",\n",
      "                                \"outlier_prob\": 0.0001\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    /* Quantization parameters for activations */\n",
      "                    \"activations\": {\n",
      "                        \"bits\": 8,                  // Number of quantization bits\n",
      "                        \"mode\": \"asymmetric\",       // Quantization mode\n",
      "                        \"granularity\": \"pertensor\", // Granularity: one scale for output tensor.\n",
      "                        /* Parameters specify how to calculate the minimum and maximum of quantization range */\n",
      "                        \"range_estimator\": {\n",
      "                            \"preset\": \"quantile\",\n",
      "                            /* OR */\n",
      "                            /* minimum of quantization range */\n",
      "                            \"min\": {\n",
      "                                \"clipping_value\": 0, // Threshold for min statistic value clipping (lower bound)\n",
      "                                \"aggregator\": \"mean\",  // Batch aggregation type [mean, max, min, median, mean_no_outliers, median_no_outliers, hl_estimator]\n",
      "                                \"type\": \"quantile\",    // Estimator type [min, max, abs_max, quantile, abs_quantile]\n",
      "                                \"outlier_prob\": 0.0001 // Outlier probability: estimator consider samples which\n",
      "                            },\n",
      "                            /* maximum of quantization range */\n",
      "                            \"max\": {\n",
      "                                \"clipping_value\": 6, // Threshold for max statistic value clipping (upper bound)\n",
      "                                \"aggregator\": \"mean\", // Batch aggregation type [mean, max, min, median, mean_no_outliers, median_no_outliers, hl_estimator]\n",
      "                                \"type\": \"quantile\",\n",
      "                                \"outlier_prob\": 0.0001\n",
      "                            }\n",
      "                        }\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/template_default_quantization.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BWYdrOrqQ_16"
   },
   "source": [
    "It contains 3 main sections: \"model\", \"engine\", \"compression\".\n",
    "\n",
    "\"model\" is a simplest, it keeps model name and path to IR.\n",
    "\n",
    "\"engine\" includes information of how the model will be executed. There are 2 modes: \"simplified\" - to run basic scenarios to roughly estimate performance gain and doesn't require dataset labels; and \"accuracy checker\" - allows to produce more accurate quantized model, allows to tune the image preprocessing, reading, etc., labeled dataset required.\n",
    "\n",
    "\"compression\" section should have all needed optimization algorithm information. \n",
    "\n",
    "More detailed information is available at the README.md file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdmPR7dVQ_16",
    "outputId": "e336e88f-c265-4630-a2fe-8485a7eb2a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Configuration File Description\n",
      "\n",
      "The toolkit is designed to work with the configuration file where all the parameters required for the optimization are specified. These parameters are organized as a dictionary and stored in\n",
      "a JSON file. JSON file allows using comments that are supported by the `jstyleson` Python* package.\n",
      "Logically all parameters are divided into three groups:\n",
      "- **Model parameters** that are related to the model definition (e.g. model name, model path, etc.)\n",
      "- **Engine parameters** that define parameters of the engine which is responsible for the model inference and data preparation used for optimization and evaluation (e.g. preprocessing parameters, dataset path, etc.)\n",
      "- **Compression parameters** that are related to the optimization algorithm (e.g. algorithm name and specific parameters)\n",
      "\n",
      "## Model Parameters\n",
      "\n",
      "This section contains only three parameters:\n",
      "- `\"model_name\"` - string parameter that defines a model name, e.g. `\"MobileNetV2\"`\n",
      "- `\"model\"` - string parameter that defines the path to an input model topology (.xml)\n",
      "- `\"weights\"` - string parameter that defines the path to an input model weights (.bin)\n",
      "\n",
      "## Engine Parameters\n",
      "\n",
      "There are two engine types in Post-Training Optimization Tool.\n",
      " - **AccuracyChecker engine**. It relies on the [Deep Learning Accuracy Validation Framework](./tools/accuracy_checker/README.md) (AccuracyChecker) when inferencing DL models and working with datasets.\n",
      " The benefit of this mode is you can compute accuracy in case you've annotations. And perform accuracy aware algorithms family.\n",
      " There are two options to define engine parameters in that mode:\n",
      "    - Refer to the existing AccuracyChecker configuration file which is represented by the YAML file. It can be a file used for full-precision model validation. In this case, only one parameter should be\n",
      "    defined:\n",
      "        - `\"config\"` - Path to the AccuracyChecker configuration file\n",
      "    - Define all the required AccuracyChecker parameters directly in the JSON file. For more details, refer to the corresponding AccuracyChecker information and examples of configuration files provided with the tool:\n",
      "        - For the SE-ResNet-50 model: `<INSTALL_DIR>/deployment_tools/tools/post_training_optimization_toolkit/configs/examples/quantization/classification/se_resnet50_pytorch_int8.json)`\n",
      "        - For the SSD-MobileNet model: `<INSTALL_DIR>/deployment_tools/tools/post_training_optimization_toolkit/configs/examples/quantization/object_detection/ssd_mobilenetv1_int8.json)`\n",
      "- **Simplified**. It does not use the AccuracyChecker tool and annotation. To measure accuracy, you should implement your own pipeline similar to the sample\n",
      "or run the evaluation script from the tool folder. if your model and dataset are supported by the AccuracyChecker. If you use the evaluation script, you should also define an AccuracyChecker config.\n",
      "    - To run the simplified mode, define engine section similar to this [example](configs/examples/quantization/classification/mobilenetV2_tf_int8_simple_mode.json).\n",
      "\n",
      "## Compression Parameters\n",
      "\n",
      "This section defines optimization algorithms and their parameters. For more details about parameters of the concrete optimization algorithm, please refer to the corresponding\n",
      "[documentation](./compression/algorithms/quantization/README.md).\n",
      "\n",
      "## Examples of the Configuration File\n",
      "\n",
      "For a quick start, several examples of configuration files for popular DL models are provided. The configuration files located in the `<INSTALL_DIR>/deployment_tools/tools/post_training_optimization_toolkit/configs/examples` folder, where `<INSTALL_DIR>` is the directory where Intel&reg; Distribution of OpenVINO&trade; toolkit is installed. For details on how to run the Post-Training Optimization Tool with a sample configuration file, see the [instructions](./configs/examples/README.md).\n"
     ]
    }
   ],
   "source": [
    "!cat /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8iG47V-Q_19"
   },
   "source": [
    "Let's run POT in different modes, compare them and practice with POT configs creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLXh4lfWQ_1-"
   },
   "source": [
    "## Step 2. Rough INT8 performance estimation (simplified mode). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkymYgMkQ_1_"
   },
   "source": [
    "Let's imagine that we have the model and we're not satisfied with it's performance level. Low precision quantization is one of the optimization options. But not all models are well-qunatizable - sometimes the performance gain can be insignificant and use this approach is a waste of the time. It really depends on the workload model/data etc.. You can quickly check whether it's worth to apply quantyzation technique or not using \"simplified mode\". Let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gs7r4r0wQ_1_"
   },
   "source": [
    "First of all, let's estimate the performance of the full precision (FP32) model using benchmark app. Benchmark app is a special tool recommended for performance estimation. This is how to work with the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G3Vu06trQ_2A",
    "outputId": "cdc780b2-ee4d-4610-9922-35f31c08cf22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "usage: benchmark_app.py [-h [HELP]] [-i PATHS_TO_INPUT [PATHS_TO_INPUT ...]]\n",
      "                        -m PATH_TO_MODEL [-d TARGET_DEVICE]\n",
      "                        [-l PATH_TO_EXTENSION] [-c PATH_TO_CLDNN_CONFIG]\n",
      "                        [-api {sync,async}] [-niter NUMBER_ITERATIONS]\n",
      "                        [-nireq NUMBER_INFER_REQUESTS] [-b BATCH_SIZE]\n",
      "                        [-stream_output [STREAM_OUTPUT]] [-t TIME]\n",
      "                        [-progress [PROGRESS]] [-nstreams NUMBER_STREAMS]\n",
      "                        [-nthreads NUMBER_THREADS] [-pin {YES,NO,NUMA}]\n",
      "                        [--exec_graph_path EXEC_GRAPH_PATH]\n",
      "                        [-pc [PERF_COUNTS]]\n",
      "                        [--report_type {no_counters,average_counters,detailed_counters}]\n",
      "                        [--report_folder REPORT_FOLDER]\n",
      "\n",
      "Options:\n",
      "  -h [HELP], --help [HELP]\n",
      "                        Show this help message and exit.\n",
      "  -i PATHS_TO_INPUT [PATHS_TO_INPUT ...], --paths_to_input PATHS_TO_INPUT [PATHS_TO_INPUT ...]\n",
      "                        Optional. Path to a folder with images and/or binaries\n",
      "                        or to specific image or binary file.\n",
      "  -m PATH_TO_MODEL, --path_to_model PATH_TO_MODEL\n",
      "                        Required. Path to an .xml file with a trained model.\n",
      "  -d TARGET_DEVICE, --target_device TARGET_DEVICE\n",
      "                        Optional. Specify a target device to infer on (the\n",
      "                        list of available devices is shown below). Default\n",
      "                        value is CPU. Use '-d HETERO:<comma separated devices\n",
      "                        list>' format to specify HETERO plugin. Use '-d\n",
      "                        MULTI:<comma separated devices list>' format to\n",
      "                        specify MULTI plugin. The application looks for a\n",
      "                        suitable plugin for the specified device.\n",
      "  -l PATH_TO_EXTENSION, --path_to_extension PATH_TO_EXTENSION\n",
      "                        Optional. Required for CPU custom layers. Absolute\n",
      "                        path to a shared library with the kernels\n",
      "                        implementations.\n",
      "  -c PATH_TO_CLDNN_CONFIG, --path_to_cldnn_config PATH_TO_CLDNN_CONFIG\n",
      "                        Optional. Required for GPU custom kernels. Absolute\n",
      "                        path to an .xml file with the kernels description.\n",
      "  -api {sync,async}, --api_type {sync,async}\n",
      "                        Optional. Enable using sync/async API. Default value\n",
      "                        is async.\n",
      "  -niter NUMBER_ITERATIONS, --number_iterations NUMBER_ITERATIONS\n",
      "                        Optional. Number of iterations. If not specified, the\n",
      "                        number of iterations is calculated depending on a\n",
      "                        device.\n",
      "  -nireq NUMBER_INFER_REQUESTS, --number_infer_requests NUMBER_INFER_REQUESTS\n",
      "                        Optional. Number of infer requests. Default value is\n",
      "                        determined automatically for device.\n",
      "  -b BATCH_SIZE, --batch_size BATCH_SIZE\n",
      "                        Optional. Batch size value. If not specified, the\n",
      "                        batch size value is determined from Intermediate\n",
      "                        Representation\n",
      "  -stream_output [STREAM_OUTPUT]\n",
      "                        Optional. Print progress as a plain text. When\n",
      "                        specified, an interactive progress bar is replaced\n",
      "                        with a multi-line output.\n",
      "  -t TIME, --time TIME  Optional. Time in seconds to execute topology.\n",
      "  -progress [PROGRESS]  Optional. Show progress bar (can affect performance\n",
      "                        measurement). Default values is 'False'.\n",
      "  -nstreams NUMBER_STREAMS, --number_streams NUMBER_STREAMS\n",
      "                        Optional. Number of streams to use for inference on\n",
      "                        the CPU/GPU in throughput mode (for HETERO and MULTI\n",
      "                        device cases use format\n",
      "                        <device1>:<nstreams1>,<device2>:<nstreams2> or just\n",
      "                        <nstreams>). Default value is determined automatically\n",
      "                        for a device. Please note that although the automatic\n",
      "                        selection usually provides a reasonable performance,\n",
      "                        it still may be non - optimal for some cases,\n",
      "                        especially for very small networks. See samples README\n",
      "                        for more details.\n",
      "  -nthreads NUMBER_THREADS, --number_threads NUMBER_THREADS\n",
      "                        Number of threads to use for inference on the CPU\n",
      "                        (including HETERO and MULTI cases).\n",
      "  -pin {YES,NO,NUMA}, --infer_threads_pinning {YES,NO,NUMA}\n",
      "                        Optional. Enable threads->cores ('YES' is default\n",
      "                        value), threads->(NUMA)nodes ('NUMA') or completely\n",
      "                        disable ('NO')CPU threads pinning for CPU-involved\n",
      "                        inference.\n",
      "  --exec_graph_path EXEC_GRAPH_PATH\n",
      "                        Optional. Path to a file where to store executable\n",
      "                        graph information serialized.\n",
      "  -pc [PERF_COUNTS], --perf_counts [PERF_COUNTS]\n",
      "                        Optional. Report performance counters.\n",
      "  --report_type {no_counters,average_counters,detailed_counters}\n",
      "                        Optional. Enable collecting statistics report.\n",
      "                        \"no_counters\" report contains configuration options\n",
      "                        specified, resulting FPS and latency.\n",
      "                        \"average_counters\" report extends \"no_counters\" report\n",
      "                        and additionally includes average PM counters values\n",
      "                        for each layer from the network. \"detailed_counters\"\n",
      "                        report extends \"average_counters\" report and\n",
      "                        additionally includes per-layer PM counters and\n",
      "                        latency for each executed infer request.\n",
      "  --report_folder REPORT_FOLDER\n",
      "                        Optional. Path to a folder where statistics report is\n",
      "                        stored.\n",
      "\n",
      "Available target devices:   CPU  GNA\n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xGfckD_Q_2C",
    "outputId": "010db970-e61b-465c-9c9d-bb0e008f15b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. \n",
      "[Step 2/11] Loading Inference Engine\n",
      "[ INFO ] InferenceEngine:\n",
      "         API version............. 2.1.2020.3.0-3467-15f2c61a-releases/2020/3\n",
      "[ INFO ] Device info\n",
      "         CPU\n",
      "         MKLDNNPlugin............ version 2.1\n",
      "         Build................... 2020.3.0-3467-15f2c61a-releases/2020/3\n",
      "\n",
      "[Step 3/11] Reading the Intermediate Representation network\n",
      "[ INFO ] Read network took 19.17 ms\n",
      "[Step 4/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Network batch size: 1\n",
      "[Step 5/11] Configuring input of the model\n",
      "[Step 6/11] Setting device configuration\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Load network took 44.00 ms\n",
      "[Step 8/11] Setting optimal runtime parameters\n",
      "[Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[ INFO ] Network input 'data' precision U8, dimensions (NCHW): 1 3 32 32\n",
      "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[ INFO ] Infer Request 0 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 1 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 2 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 3 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 4 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 5 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 6 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 7 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 8 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 9 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 10 inference requests using 10 streams for CPU, limits: 60000 ms duration)\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:      4347230 iterations\n",
      "Duration:   60000.22 ms\n",
      "Latency:    0.12 ms\n",
      "Throughput: 72453.56 FPS\n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py -m IR/SampLeNet.xml -i cifar-10-python/01_cat.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zK1enU25Q_2E"
   },
   "source": [
    "That's great throughput and latency numbers, but let's see can we improve it or not. So, we need to create POT config for that purpose. To use simplified mode we need specify \"type\": \"simplified\" and \"data_source\": \"path/to/the/dataset\" fields at the compression section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eDcV_58Q_2F",
    "outputId": "a8b88919-f948-475e-9188-66068b9dfa54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": {\n",
      "    \"model_name\": \"SampLeNet_simplified\",\n",
      "    \"model\": \"IR/SampLeNet.xml\",\n",
      "    \"weights\": \"IR/SampLeNet.bin\"\n",
      "  },\n",
      "  \"engine\": {\n",
      "    \"type\": \"simplified\",\n",
      "    \"data_source\": \"cifar-10-python\"\n",
      "  },\n",
      "  \"compression\": {\n",
      "    \"target_device\": \"CPU\",\n",
      "    \"algorithms\": [\n",
      "      {\n",
      "        \"name\": \"DefaultQuantization\",\n",
      "        \"params\": {\n",
      "          \"preset\": \"performance\",\n",
      "          \"stat_subset_size\": 300\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat basic_materials/SampLeNet_simplified.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run quantization in simplified mode. Using \"-d\" option to simplify results storage and further reuse of the models by benchmark app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z65mUAmhQ_2G",
    "outputId": "4e9fdf70-50c9-4b3e-d530-dd7a58cdcde9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:app.run:Output log dir: ./results\n",
      "INFO:app.run:Creating pipeline:\n",
      " Algorithm: DefaultQuantization\n",
      " Parameters:\n",
      "\tpreset                     : performance\n",
      "\tstat_subset_size           : 300\n",
      "\ttarget_device              : CPU\n",
      "\texec_log_dir               : ./results\n",
      " ===========================================================================\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : DefaultQuantization\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.pipeline.pipeline:Start algorithm: DefaultQuantization\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : ActivationChannelAlignment\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : MinMaxQuantization,FastBiasCorrection\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.pipeline.pipeline:Finished: DefaultQuantization\n",
      " ===========================================================================\n"
     ]
    }
   ],
   "source": [
    "!pot -c basic_materials/SampLeNet_simplified.json -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6SPfe_ZQ_2I"
   },
   "source": [
    "Rough estimation of how we can benefit from INT8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HW2SSfCpQ_2J",
    "outputId": "15acaf46-1475-4632-86f4-2f3bbbd80d70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. \n",
      "[Step 2/11] Loading Inference Engine\n",
      "[ INFO ] InferenceEngine:\n",
      "         API version............. 2.1.2020.3.0-3467-15f2c61a-releases/2020/3\n",
      "[ INFO ] Device info\n",
      "         CPU\n",
      "         MKLDNNPlugin............ version 2.1\n",
      "         Build................... 2020.3.0-3467-15f2c61a-releases/2020/3\n",
      "\n",
      "[Step 3/11] Reading the Intermediate Representation network\n",
      "[ INFO ] Read network took 14.82 ms\n",
      "[Step 4/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Network batch size: 1\n",
      "[Step 5/11] Configuring input of the model\n",
      "[Step 6/11] Setting device configuration\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Load network took 73.93 ms\n",
      "[Step 8/11] Setting optimal runtime parameters\n",
      "[Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[ INFO ] Network input 'data' precision U8, dimensions (NCHW): 1 3 32 32\n",
      "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[ INFO ] Infer Request 0 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 1 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 2 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 3 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 4 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 5 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 6 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 7 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 8 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 9 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 10 inference requests using 10 streams for CPU, limits: 60000 ms duration)\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:      5430190 iterations\n",
      "Duration:   60000.17 ms\n",
      "Latency:    0.08 ms\n",
      "Throughput: 90502.91 FPS\n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py -m results/optimized/SampLeNet_simplified.xml -i cifar-10-python/01_cat.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Measuring accuracy of FP32 model. Accuracy Checker configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bf8yQMu7Q_2M"
   },
   "source": [
    "Looks like we can dramatically accelerate our workload. But what's about performance. If we're able to run the model with benchmark it's not guaranteed that the inference result is correct. To confirm that the model output is correct you can check visually/manually, but we're offering Accuracy Checker tool which estimates accuracy metrics of given model on given dataset. It can be used directly using accuracy_check alias or call it from the POT config. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kL6p_E4Q_2M",
    "outputId": "69a91824-3743-4d12-a8e5-15f7e1ac6627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:42:42 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\n",
      "02:42:42 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\n",
      "02:42:42 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\n",
      "02:42:42 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\n",
      "02:42:42 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\n",
      "02:42:42 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\n",
      "usage: accuracy_check [-h] [-d DEFINITIONS] -c CONFIG [-m MODELS [MODELS ...]]\n",
      "                      [-s SOURCE] [-a ANNOTATIONS] [-e EXTENSIONS]\n",
      "                      [--cpu_extensions_mode {avx512,avx2,sse4}]\n",
      "                      [-b BITSTREAMS]\n",
      "                      [--stored_predictions STORED_PREDICTIONS]\n",
      "                      [-C CONVERTED_MODELS] [-M MODEL_OPTIMIZER]\n",
      "                      [--tf_custom_op_config_dir TF_CUSTOM_OP_CONFIG_DIR]\n",
      "                      [--transformations_config_dir TRANSFORMATIONS_CONFIG_DIR]\n",
      "                      [--tf_obj_detection_api_pipeline_config_path TF_OBJ_DETECTION_API_PIPELINE_CONFIG_PATH]\n",
      "                      [--progress PROGRESS]\n",
      "                      [--progress_interval PROGRESS_INTERVAL]\n",
      "                      [-tf TARGET_FRAMEWORK]\n",
      "                      [-td TARGET_DEVICES [TARGET_DEVICES ...]]\n",
      "                      [-tt TARGET_TAGS [TARGET_TAGS ...]] [-l LOG_FILE]\n",
      "                      [--ignore_result_formatting IGNORE_RESULT_FORMATTING]\n",
      "                      [-am AFFINITY_MAP] [--aocl AOCL]\n",
      "                      [--vpu_log_level {LOG_NONE,LOG_WARNING,LOG_INFO,LOG_DEBUG}]\n",
      "                      [--deprecated_ir_v7 DEPRECATED_IR_V7]\n",
      "                      [-dc DEVICE_CONFIG] [--async_mode ASYNC_MODE]\n",
      "                      [--num_requests NUM_REQUESTS] [--csv_result CSV_RESULT]\n",
      "                      [--model_is_blob MODEL_IS_BLOB]\n",
      "\n",
      "Deep Learning accuracy validation framework\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -d DEFINITIONS, --definitions DEFINITIONS\n",
      "                        path to the yml file with definitions\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        path to the yml file with local configuration\n",
      "  -m MODELS [MODELS ...], --models MODELS [MODELS ...]\n",
      "                        prefix path to the models and weights\n",
      "  -s SOURCE, --source SOURCE\n",
      "                        prefix path to the data source\n",
      "  -a ANNOTATIONS, --annotations ANNOTATIONS\n",
      "                        prefix path to the converted annotations and datasets\n",
      "                        meta data\n",
      "  -e EXTENSIONS, --extensions EXTENSIONS\n",
      "                        prefix path to extensions folder\n",
      "  --cpu_extensions_mode {avx512,avx2,sse4}\n",
      "                        specified preferable set of processor instruction for\n",
      "                        automatic searching cpu extension lib\n",
      "  -b BITSTREAMS, --bitstreams BITSTREAMS\n",
      "                        prefix path to bitstreams folder\n",
      "  --stored_predictions STORED_PREDICTIONS\n",
      "                        path to file with saved predictions. Used for\n",
      "                        development\n",
      "  -C CONVERTED_MODELS, --converted_models CONVERTED_MODELS\n",
      "                        directory to store Model Optimizer converted models.\n",
      "                        Used for DLSDK launcher only\n",
      "  -M MODEL_OPTIMIZER, --model_optimizer MODEL_OPTIMIZER\n",
      "                        path to model optimizer directory\n",
      "  --tf_custom_op_config_dir TF_CUSTOM_OP_CONFIG_DIR\n",
      "                        path to directory with tensorflow custom operation\n",
      "                        configuration files for model optimizer\n",
      "  --transformations_config_dir TRANSFORMATIONS_CONFIG_DIR\n",
      "                        path to directory with Model Optimizer transformations\n",
      "                        configuration files\n",
      "  --tf_obj_detection_api_pipeline_config_path TF_OBJ_DETECTION_API_PIPELINE_CONFIG_PATH\n",
      "                        path to directory with tensorflow object detection api\n",
      "                        pipeline configuration files for model optimizer\n",
      "  --progress PROGRESS   progress reporter. You can select bar or print\n",
      "  --progress_interval PROGRESS_INTERVAL\n",
      "                        interval for update progress if selected *print*\n",
      "                        progress.\n",
      "  -tf TARGET_FRAMEWORK, --target_framework TARGET_FRAMEWORK\n",
      "                        framework for infer\n",
      "  -td TARGET_DEVICES [TARGET_DEVICES ...], --target_devices TARGET_DEVICES [TARGET_DEVICES ...]\n",
      "                        Space separated list of devices for infer\n",
      "  -tt TARGET_TAGS [TARGET_TAGS ...], --target_tags TARGET_TAGS [TARGET_TAGS ...]\n",
      "                        Space separated list of launcher tags for infer\n",
      "  -l LOG_FILE, --log_file LOG_FILE\n",
      "                        file for additional logging results\n",
      "  --ignore_result_formatting IGNORE_RESULT_FORMATTING\n",
      "                        allow to get raw metrics results without data\n",
      "                        formatting\n",
      "  -am AFFINITY_MAP, --affinity_map AFFINITY_MAP\n",
      "                        prefix path to the affinity maps\n",
      "  --aocl AOCL           path to aocl executable for FPGA bitstream programming\n",
      "  --vpu_log_level {LOG_NONE,LOG_WARNING,LOG_INFO,LOG_DEBUG}\n",
      "                        log level for VPU devices\n",
      "  --deprecated_ir_v7 DEPRECATED_IR_V7\n",
      "                        Allow generation IR v7 via Model Optimizer\n",
      "  -dc DEVICE_CONFIG, --device_config DEVICE_CONFIG\n",
      "                        Inference Engine device specific config file\n",
      "  --async_mode ASYNC_MODE\n",
      "                        Allow evaluation in async mode\n",
      "  --num_requests NUM_REQUESTS\n",
      "                        the number of infer requests\n",
      "  --csv_result CSV_RESULT\n",
      "                        file for results writing\n",
      "  --model_is_blob MODEL_IS_BLOB\n",
      "                        the tip for automatic model search to use blob for\n",
      "                        dlsdk launcher\n"
     ]
    }
   ],
   "source": [
    "!accuracy_check -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xN_sav7eQ_2O"
   },
   "source": [
    "It was how to use Accuracy Checker directly, it uses it's own .yaml configuration files. But to avoid producing too many configs, here we'll call it from \"engine\" section at the same POT config. Let's change it like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Vm_G2yHQ_2O",
    "outputId": "537c6df1-1c73-44a0-c963-b64be3417c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": {\n",
      "    \"model_name\": \"SampLeNet_FP32\",\n",
      "    \"model\": \"IR/SampLeNet.xml\",\n",
      "    \"weights\": \"IR/SampLeNet.bin\"\n",
      "  },\n",
      "  \"engine\": {\n",
      "    \"launchers\": [\n",
      "      {\n",
      "        \"framework\": \"dlsdk\",\n",
      "        \"device\": \"CPU\",\n",
      "        \"adapter\": \"classification\"\n",
      "      }\n",
      "    ],\n",
      "    \"datasets\": [\n",
      "      {\n",
      "        \"name\": \"classification_dataset\",\n",
      "        \"data_source\": \"cifar-10-python\",\n",
      "        \"annotation_conversion\": {\n",
      "          \"converter\": \"cifar\",\n",
      "          \"data_batch_file\": \"cifar-10-batches-py/test_batch\",\n",
      "          \"convert_images\": true,\n",
      "          \"converted_images_dir\": \"cifar-10-python\",\n",
      "          \"num_classes\": 10\n",
      "        },\n",
      "        \"reader\": \"opencv_imread\",\n",
      "        \"preprocessing\": [\n",
      "            {\n",
      "                \"type\": \"bgr_to_rgb\"\n",
      "            }\n",
      "        ],\n",
      "        \"metrics\": [\n",
      "          {\n",
      "            \"name\": \"accuracy@top1\",\n",
      "            \"type\": \"accuracy\",\n",
      "            \"top_k\": 1\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"accuracy@top5\",\n",
      "            \"type\": \"accuracy\",\n",
      "            \"top_k\": 5\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat basic_materials/SampLeNet_FP32.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dyklDaCQ_2R"
   },
   "source": [
    "**Accuracy Checker configuration files have the following parts:**\n",
    "\n",
    "* **Launchers** are the inference backend. It can be OpenVINO Inference Engine or frameworks like Tensorflow, Pytorch, ONNX Runtime, etc. So, choosing different launchers you can compare an accuracy of the model inferred using OpenVINO and by the framework it was trained. Here you can also specify inference device. At the example above we're using Inference Engine Launcher on CPU. Click [here](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_README.html) to see full launchers list.\n",
    "\n",
    "   **Please note that for quantization via POT OpenVINO Inference Engine is  only available inference backend.**\n",
    "\n",
    "* **Adapters**. Adapter converts network infer output to metric specific format. [Here are available adapters](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_accuracy_checker_adapters_README.html). We have simple classification model, so, our adapter is \"classification\". \n",
    "\n",
    "* **Annotation Converters**. Today there are thousands of datasets and each has its own annotation format. AC uses it's own internal dataset annotation format. So, annotation converter converts the dataset annotation from its format to AC one. [Check supported datasets](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_accuracy_checker_annotation_converters_README.html)\n",
    "    At this example we're using cifar10 - let's specify that, and don't forget to put the number of classes.\n",
    "\n",
    "* **Readers**. It's how images from the dataset will be read. Look for [implemented readers](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_accuracy_checker_data_readers_README.html)\n",
    "We're going to read RGB images in .png format - several readers from the list works for us. OpenCV imread is the default one, we'll it use because the same reader was used on the training stage - so, that makes our experiments more precise.\n",
    "\n",
    "* [Preprocesors](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_accuracy_checker_preprocessor_README.html) and [Postprocessors](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_accuracy_checker_postprocessor_README.html) - additional calibration datased pre- and post-processing can be added if appropriate (if that's done in model training). The most popular preprocessing is resize images to fit model input shape. In our case it is not required because images already have the same size like model input. We already added some normalization (scale and mean values) on Model Optimizer side, so, we don't need to put it here. If we forget to add \"--mean\" and -\"--scale\" at the MO command line, we can do it here. Only one preprocessing we should add is BGR to RGB conversion, because topology is trained on RGB images, but OpenCV reader (opencv_imread) reads in BGR.\n",
    "\n",
    "* **Metrics**. It's the way of accuracy measuring. Different CV tasks like classification, detection, segmentation, etc has different approaches to measure accuracy. You can choose among [these metrics](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_accuracy_checker_metrics_README.html).\n",
    "Our SampLeNet is a classification model, cifar is classification dataset, so, let's apply most popular classification metrics: `top1` and `top5`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7FyrBMKQ_2S"
   },
   "source": [
    "To calculate accuracy of full precision model using POT and Accuracy Checker - leave empty \"compression\" section at the POT config and add \"-e\" (evaluation) parameter to the command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FS6UQnmwQ_2S",
    "outputId": "8c218bec-bb98-4d97-a901-20059eb82df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:app.run:Output log dir: ./results\n",
      "INFO:app.run:Creating pipeline:\n",
      " ===========================================================================\n",
      "IE version: 2.1.2020.3.0-3467-15f2c61a-releases/2020/3\n",
      "Loaded CPU plugin version:\n",
      "    CPU - MKLDNNPlugin: 2.1.2020.3.0-3467-15f2c61a-releases/2020/3\n",
      "INFO:compression.pipeline.pipeline:Evaluation of generated model\n",
      "INFO:compression.engines.ac_engine:Start inference on the whole dataset\n",
      "Total dataset size: 10000\n",
      "1000 / 10000 processed in 1.995s\n",
      "2000 / 10000 processed in 1.924s\n",
      "3000 / 10000 processed in 1.978s\n",
      "4000 / 10000 processed in 1.951s\n",
      "5000 / 10000 processed in 1.979s\n",
      "6000 / 10000 processed in 1.945s\n",
      "7000 / 10000 processed in 1.949s\n",
      "8000 / 10000 processed in 1.951s\n",
      "9000 / 10000 processed in 1.941s\n",
      "10000 / 10000 processed in 1.932s\n",
      "10000 objects processed in 19.547 seconds\n",
      "INFO:compression.engines.ac_engine:Inference finished\n",
      "INFO:app.run:accuracy@top1              : 0.7502\n",
      "INFO:app.run:accuracy@top5              : 0.9822\n"
     ]
    }
   ],
   "source": [
    "!pot -c basic_materials/SampLeNet_FP32.json -e -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlKcGyCFQ_2U"
   },
   "source": [
    "FP32 model accuracy level is exactly the same with Caffe output, so, our model is running correctly. Let's quantize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Running calibration algorithms. Evaluating it's accuracy and perfromance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0sKl8RWQ_2U"
   },
   "source": [
    "Currently (OpenVINO 2020.2) there are 2 \"production quality\" quantization algorithms: DefaultQauntization and AccuracyAwareQuantization. Let's read how do they work at the README file below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJ9F_NuAQ_2V",
    "outputId": "75d4348b-921d-4981-8431-3fb9eee86a43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Quantization\n",
      "The primary optimization feature of the toolkit is a uniform quantization. In general, this method supports an arbitrary number of bits (>=2) which is used to represent weights and activations.\n",
      "During the quantization process, the so-called `FakeQuantize` operations are inserted into the model graph automatically based on the predefined hardware target in order to produce the most\n",
      "hardware-friendly optimized model. After that, different quantization algorithms can tune the `FakeQuantize` parameters or remove some operations in order to meet the accuracy criteria. The resulting\n",
      "\"fakequantized\" models can be interpreted and transformed to real low-precision models at runtime getting real performance improvement.\n",
      "\n",
      "## Quantization algorithms\n",
      "The toolkit provides multiple quantization and auxiliary algorithms which help to restore the accuracy after quantizing weights and activations. Potentially, algorithms can form independent\n",
      "optimization pipelines that can be applied to quantize one or another model. However, the only two following quantization algorithms for 8-bits precision are verified and recommended for use to get stable and confident results for DNN model quantization:\n",
      "*  **DefaultQuantization** - Used as a default method to get fast but in most cases accurate results for 8-bits quantization. For details, see [DefaultQuantization Algorithm](./compression/algorithms/quantization/default/README.md) documentation.\n",
      "\n",
      "*  **AccuracyAwareQuantization** - Allows staying at the pre-defined range of accuracy drop after the quantization while sacrificing performance improvement. It may require more time for quantization. For details, see the [AccuracyAwareQuantization Algorithm](./compression/algorithms/quantization/accuracy_aware/README.md) documentation.\n",
      "\n",
      "## Quantization formula\n",
      "Quantization is parametrized by clamping range and number of quantization levels:\n",
      "\n",
      "\\f[\n",
      "output = \\frac{\\left\\lfloor (clamp(input; input\\_low, input\\_high)-input\\_low)  *s\\right \\rceil}{s} + input\\_low\\\\\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "clamp(input; input\\_low, input\\_high) = min(max(input, input\\_low), input\\_high)))\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "s=\\frac{levels-1}{input\\_high - input\\_low}\n",
      "\\f]\n",
      "\n",
      "`input_low` and `input_high` represents the quantization range and \\f[\\left\\lfloor\\cdot\\right \\rceil\\f] \n",
      "denotes rounding to the nearest integer.\n",
      "\n",
      "The toolkit supports two quantization modes: symmetric and asymmetric. The main difference between them is that in the case of the symmetric mode the floating-point zero is mapped directly to\n",
      "integer zero. For asymmetric mode it can be any integer number but in any case the floating-point zero is mapped directly to the quant without rounding error.\n",
      "\n",
      "####  Symmetric quantization\n",
      "\n",
      "The formula is parametrized by the `scale` parameter that is tuned during quantization process:\n",
      "\n",
      "\\f[\n",
      "input\\_low=scale*\\frac{level\\_low}{level\\_high}\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "input\\_high=scale\n",
      "\\f]\n",
      "\n",
      "\n",
      "Where `level_low` and `level_high` represent the range of the discrete signal.\n",
      "* For weights:\n",
      "\n",
      "\\f[\n",
      "level\\_low=-2^{bits-1}+1\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "level\\_high=2^{bits-1}-1\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "levels=255\n",
      "\\f]\n",
      "\n",
      "* For unsigned activations:\n",
      "\n",
      "\\f[\n",
      "level\\_low=0\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "level\\_high=2^{bits}-1\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "levels=256\n",
      "\\f]\n",
      "\n",
      "*For signed activations:\n",
      "\n",
      "\\f[\n",
      "level\\_low=-2^{bits-1}\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "level\\_high=2^{bits-1}-1\n",
      "\\f]\n",
      "\n",
      "\n",
      "\\f[\n",
      "levels=256\n",
      "\\f]\n",
      "\n",
      "####  Asymmetric quantization\n",
      "\n",
      "The quantization formula is parametrized by `input_low` and `input_range` that are tunable parameters:\n",
      "\n",
      "\\f[\n",
      "input\\_high=input\\_low + input\\_range\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "levels=256\n",
      "\\f]\n",
      "\n",
      "For weights and activations the following quantization mode is applied:\n",
      "\n",
      "\\f[\n",
      "{input\\_low}' = min(input\\_low, 0)\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "{input\\_high}' = max(input\\_high, 0)\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "ZP= \\left\\lfloor \\frac{-{input\\_low}'*(levels-1)}{{input\\_high}'-{input\\_low}'} \\right \\rceil \n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "{input\\_high}''=\\frac{ZP-levels+1}{ZP}*{input\\_low}'\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "{input\\_low}''=\\frac{ZP}{ZP-levels+1}*{input\\_high}'\n",
      "\\f]\n",
      "\n",
      "\\f[\n",
      "{input\\_low,input\\_high} = \\begin{cases} {input\\_low}',{input\\_high}', & ZP \\in $\\{0,levels-1\\}$ \\\\ {input\\_low}',{input\\_high}'', & {input\\_high}'' - {input\\_low}' > {input\\_high}' - {input\\_low}'' \\\\ {input\\_low}'',{input\\_high}', & {input\\_high}'' - {input\\_low}' <= {input\\_high}' - {input\\_low}''\\\\ \\end{cases}\n",
      "\\f]"
     ]
    }
   ],
   "source": [
    "!cat /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/compression/algorithms/quantization/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1 Running DefaultQuantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ElFg5QXTQ_2W"
   },
   "source": [
    "Let's apply DefaultQuantization, filing \"compression\" section as at the config below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "acmycRKPQ_2X",
    "outputId": "ce261835-0a38-4676-a579-12fa2f71c4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": {\n",
      "    \"model_name\": \"SampLeNet_DefaultQuantization\",\n",
      "    \"model\": \"IR/SampLeNet.xml\",\n",
      "    \"weights\": \"IR/SampLeNet.bin\"\n",
      "  },\n",
      "  \"engine\": {\n",
      "    \"launchers\": [\n",
      "      {\n",
      "        \"framework\": \"dlsdk\",\n",
      "        \"device\": \"CPU\",\n",
      "        \"adapter\": \"classification\"\n",
      "      }\n",
      "    ],\n",
      "    \"datasets\": [\n",
      "      {\n",
      "        \"name\": \"classification_dataset\",\n",
      "        \"data_source\": \"cifar-10-python\",\n",
      "        \"annotation_conversion\": {\n",
      "          \"converter\": \"cifar\",\n",
      "          \"data_batch_file\": \"cifar-10-batches-py/test_batch\",\n",
      "          \"convert_images\": true,\n",
      "          \"converted_images_dir\": \"cifar-10-python\",\n",
      "          \"num_classes\": 10\n",
      "        },\n",
      "        \"reader\": \"opencv_imread\",\n",
      "        \"preprocessing\": [\n",
      "            {\n",
      "                \"type\": \"bgr_to_rgb\"\n",
      "            }\n",
      "        ],\n",
      "        \"metrics\": [\n",
      "          {\n",
      "            \"name\": \"accuracy@top1\",\n",
      "            \"type\": \"accuracy\",\n",
      "            \"top_k\": 1\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"accuracy@top5\",\n",
      "            \"type\": \"accuracy\",\n",
      "            \"top_k\": 5\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "    \"compression\": {\n",
      "    \"target_device\": \"CPU\",\n",
      "    \"algorithms\": [\n",
      "      {\n",
      "        \"name\": \"DefaultQuantization\",\n",
      "        \"params\": {\n",
      "          \"preset\": \"performance\",\n",
      "          \"stat_subset_size\": 300\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat basic_materials/SampLeNet_DefaultQuantization.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCNntEU5Q_2Y"
   },
   "source": [
    "Running it, again using \"-e\" option to see accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_SHVc-fMQ_2Z",
    "outputId": "7a7dd9a0-abf0-406a-c833-db69e6b522d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:app.run:Output log dir: ./results\n",
      "INFO:app.run:Creating pipeline:\n",
      " Algorithm: DefaultQuantization\n",
      " Parameters:\n",
      "\tpreset                     : performance\n",
      "\tstat_subset_size           : 300\n",
      "\ttarget_device              : CPU\n",
      "\texec_log_dir               : ./results\n",
      " ===========================================================================\n",
      "IE version: 2.1.2020.3.0-3467-15f2c61a-releases/2020/3\n",
      "Loaded CPU plugin version:\n",
      "    CPU - MKLDNNPlugin: 2.1.2020.3.0-3467-15f2c61a-releases/2020/3\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : DefaultQuantization\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.pipeline.pipeline:Start algorithm: DefaultQuantization\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : ActivationChannelAlignment\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : MinMaxQuantization,FastBiasCorrection\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.pipeline.pipeline:Finished: DefaultQuantization\n",
      " ===========================================================================\n",
      "INFO:compression.pipeline.pipeline:Evaluation of generated model\n",
      "INFO:compression.engines.ac_engine:Start inference on the whole dataset\n",
      "Total dataset size: 10000\n",
      "1000 / 10000 processed in 2.090s\n",
      "2000 / 10000 processed in 2.036s\n",
      "3000 / 10000 processed in 2.074s\n",
      "4000 / 10000 processed in 2.021s\n",
      "5000 / 10000 processed in 2.026s\n",
      "6000 / 10000 processed in 2.038s\n",
      "7000 / 10000 processed in 2.038s\n",
      "8000 / 10000 processed in 1.993s\n",
      "9000 / 10000 processed in 2.051s\n",
      "10000 / 10000 processed in 2.018s\n",
      "10000 objects processed in 20.387 seconds\n",
      "INFO:compression.engines.ac_engine:Inference finished\n",
      "INFO:app.run:accuracy@top1              : 0.7414\n",
      "INFO:app.run:accuracy@top5              : 0.9809\n"
     ]
    }
   ],
   "source": [
    "!pot -c basic_materials/SampLeNet_DefaultQuantization.json -e -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PJN1rHYQ_2b"
   },
   "source": [
    "Accuracy doesn't deviate so much from FP32 model - we can stop here, but let's check AccuracyAwareAlgorithm. Just change the algo name in compression section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2 Running AccuracyAwareQuantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HSoEMFQQ_2c",
    "outputId": "9ff07aa8-1ff1-40cb-e948-4f0658e5a461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": {\n",
      "    \"model_name\": \"SampLeNet_AccuracyAware\",\n",
      "    \"model\": \"IR/SampLeNet.xml\",\n",
      "    \"weights\": \"IR/SampLeNet.bin\"\n",
      "  },\n",
      "  \"engine\": {\n",
      "    \"launchers\": [\n",
      "      {\n",
      "        \"framework\": \"dlsdk\",\n",
      "        \"device\": \"CPU\",\n",
      "        \"adapter\": \"classification\"\n",
      "      }\n",
      "    ],\n",
      "    \"datasets\": [\n",
      "      {\n",
      "        \"name\": \"classification_dataset\",\n",
      "        \"data_source\": \"cifar-10-python\",\n",
      "        \"annotation_conversion\": {\n",
      "          \"converter\": \"cifar\",\n",
      "          \"data_batch_file\": \"cifar-10-batches-py/test_batch\",\n",
      "          \"convert_images\": true,\n",
      "          \"converted_images_dir\": \"cifar-10-python\",\n",
      "          \"num_classes\": 10\n",
      "        },\n",
      "        \"reader\": \"opencv_imread\",\n",
      "        \"preprocessing\": [\n",
      "            {\n",
      "                \"type\": \"bgr_to_rgb\"\n",
      "            }\n",
      "        ],\n",
      "        \"metrics\": [\n",
      "          {\n",
      "            \"name\": \"accuracy@top1\",\n",
      "            \"type\": \"accuracy\",\n",
      "            \"top_k\": 1\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"accuracy@top5\",\n",
      "            \"type\": \"accuracy\",\n",
      "            \"top_k\": 5\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"compression\": {\n",
      "    \"target_device\": \"CPU\",\n",
      "    \"algorithms\": [\n",
      "      {\n",
      "        \"name\": \"AccuracyAwareQuantization\",\n",
      "        \"params\": {\n",
      "          \"preset\": \"performance\",\n",
      "          \"stat_subset_size\": 300\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat basic_materials/SampLeNet_AccuracyAwareQuantization.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGheA0sZQ_2e",
    "outputId": "9e3d4f98-e49d-470a-c4e1-208553ae767d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:app.run:Output log dir: ./results\n",
      "INFO:app.run:Creating pipeline:\n",
      " Algorithm: AccuracyAwareQuantization\n",
      " Parameters:\n",
      "\tpreset                     : performance\n",
      "\tstat_subset_size           : 300\n",
      "\ttarget_device              : CPU\n",
      "\texec_log_dir               : ./results\n",
      " ===========================================================================\n",
      "IE version: 2.1.2020.3.0-3467-15f2c61a-releases/2020/3\n",
      "Loaded CPU plugin version:\n",
      "    CPU - MKLDNNPlugin: 2.1.2020.3.0-3467-15f2c61a-releases/2020/3\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : AccuracyAwareQuantization\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.pipeline.pipeline:Start algorithm: AccuracyAwareQuantization\n",
      "INFO:compression.algorithms.quantization.accuracy_aware.algorithm:Start original model inference\n",
      "INFO:compression.engines.ac_engine:Start inference of 10000 images\n",
      "Total dataset size: 10000\n",
      "1000 / 10000 processed in 1.981s\n",
      "2000 / 10000 processed in 1.974s\n",
      "3000 / 10000 processed in 1.980s\n",
      "4000 / 10000 processed in 1.963s\n",
      "5000 / 10000 processed in 1.966s\n",
      "6000 / 10000 processed in 1.977s\n",
      "7000 / 10000 processed in 1.947s\n",
      "8000 / 10000 processed in 1.991s\n",
      "9000 / 10000 processed in 1.959s\n",
      "10000 / 10000 processed in 1.959s\n",
      "10000 objects processed in 19.698 seconds\n",
      "INFO:compression.engines.ac_engine:Inference finished\n",
      "INFO:compression.algorithms.quantization.accuracy_aware.algorithm:Baseline metrics: {'accuracy@top1': 0.7502, 'accuracy@top5': 0.9822}\n",
      "INFO:compression.algorithms.quantization.accuracy_aware.algorithm:Start quantization\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : ActivationChannelAlignment\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : MinMaxQuantization,FastBiasCorrection\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.algorithms.quantization.accuracy_aware.algorithm:Start compressed model inference\n",
      "INFO:compression.engines.ac_engine:Start inference of 10000 images\n",
      "Total dataset size: 10000\n",
      "1000 / 10000 processed in 1.637s\n",
      "2000 / 10000 processed in 1.710s\n",
      "3000 / 10000 processed in 1.662s\n",
      "4000 / 10000 processed in 1.705s\n",
      "5000 / 10000 processed in 1.757s\n",
      "6000 / 10000 processed in 1.748s\n",
      "7000 / 10000 processed in 1.758s\n",
      "8000 / 10000 processed in 1.725s\n",
      "9000 / 10000 processed in 1.719s\n",
      "10000 / 10000 processed in 1.737s\n",
      "10000 objects processed in 17.158 seconds\n",
      "INFO:compression.engines.ac_engine:Inference finished\n",
      "INFO:compression.algorithms.quantization.accuracy_aware.algorithm:Fully quantized metrics: {'accuracy@top1': 0.7414, 'accuracy@top5': 0.9809}\n",
      "INFO:compression.algorithms.quantization.accuracy_aware.algorithm:Accuracy drop: {'accuracy@top1': 0.00880000000000003, 'accuracy@top5': 0.0012999999999999678}\n",
      "INFO:compression.pipeline.pipeline:Finished: AccuracyAwareQuantization\n",
      " ===========================================================================\n",
      "INFO:compression.pipeline.pipeline:Evaluation of generated model\n",
      "INFO:compression.engines.ac_engine:Start inference on the whole dataset\n",
      "Total dataset size: 10000\n",
      "1000 / 10000 processed in 1.541s\n",
      "2000 / 10000 processed in 1.705s\n",
      "3000 / 10000 processed in 1.713s\n",
      "4000 / 10000 processed in 1.704s\n",
      "5000 / 10000 processed in 1.746s\n",
      "6000 / 10000 processed in 1.692s\n",
      "7000 / 10000 processed in 1.708s\n",
      "8000 / 10000 processed in 1.727s\n",
      "9000 / 10000 processed in 1.700s\n",
      "10000 / 10000 processed in 1.697s\n",
      "10000 objects processed in 16.933 seconds\n",
      "INFO:compression.engines.ac_engine:Inference finished\n",
      "INFO:app.run:accuracy@top1              : 0.7414\n",
      "INFO:app.run:accuracy@top5              : 0.9809\n"
     ]
    }
   ],
   "source": [
    "!pot -c basic_materials/SampLeNet_AccuracyAwareQuantization.json -e -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJ0fkelqQ_2m"
   },
   "source": [
    "AccuracyAwareQuantization produced INT8 model with the same accuracy level with default one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKZL7yUwQ_2m"
   },
   "source": [
    "Now it's time to compare performance level of quantized model using Benchmark App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJXIFMeMQ_2n",
    "outputId": "4703a293-7963-4825-9d41-5c421a56ba25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. \n",
      "[Step 2/11] Loading Inference Engine\n",
      "[ INFO ] InferenceEngine:\n",
      "         API version............. 2.1.2020.3.0-3467-15f2c61a-releases/2020/3\n",
      "[ INFO ] Device info\n",
      "         CPU\n",
      "         MKLDNNPlugin............ version 2.1\n",
      "         Build................... 2020.3.0-3467-15f2c61a-releases/2020/3\n",
      "\n",
      "[Step 3/11] Reading the Intermediate Representation network\n",
      "[ INFO ] Read network took 13.90 ms\n",
      "[Step 4/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Network batch size: 1\n",
      "[Step 5/11] Configuring input of the model\n",
      "[Step 6/11] Setting device configuration\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Load network took 68.42 ms\n",
      "[Step 8/11] Setting optimal runtime parameters\n",
      "[Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[ INFO ] Network input 'data' precision U8, dimensions (NCHW): 1 3 32 32\n",
      "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[ INFO ] Infer Request 0 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 1 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 2 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 3 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 4 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 5 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 6 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 7 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 8 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 9 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 10 inference requests using 10 streams for CPU, limits: 60000 ms duration)\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:      5371590 iterations\n",
      "Duration:   60000.17 ms\n",
      "Latency:    0.08 ms\n",
      "Throughput: 89526.24 FPS\n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py -m results/optimized/SampLeNet_AccuracyAware.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNORg4wyQ_2q"
   },
   "source": [
    "Looks like we have good performance gain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpdesTpcQ_2q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "POT_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
