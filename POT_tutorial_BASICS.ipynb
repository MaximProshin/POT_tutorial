{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFfNpMd0Q_1Q"
   },
   "source": [
    "# Post-training Optimization Toolkit Basics Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q8AW8FmpQ_1R"
   },
   "source": [
    "Post-training optimization Toolkit (POT) is helping to optimize the model applying different optimization teqniques like low precision qunatization and sparsity. Please refer to the documention here: https://docs.openvinotoolkit.org/latest/_README.html In that tutorial we'll concentrate on INT8 quantization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates basic capabilities of POT:\n",
    "* POT configuration files structure\n",
    "* How to run POT in simplified mode\n",
    "* How to measure accuracy of FP32, INT8 models using POT config \n",
    "* How to create your own POT config\n",
    "* How to properly benchmark the workload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pp98oZdvQ_1T"
   },
   "source": [
    "## Step 0. Prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fnIxpsAlQ_1T"
   },
   "source": [
    "To do a quantization you need pre-trained model in IR format and calibration dataset. Let's prepare both components in this step. In this tutorial, we'll use SimpLeNet - very simple model trained specially for sample purposes on cifar-10 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBE3uq-uQ_1U"
   },
   "source": [
    "### Step 0.1 Converting model to IR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SampLeNet is distributed as a part of OpenVINO and used in AccuracyChecker sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch_model\t       SampLeNet.caffemodel  samplenet-symbol.json\n",
      "samplenet-0000.params  samplenet.onnx\t     SampLeNet.xml\n",
      "SampLeNet.bin\t       samplenet.pb\n",
      "SampLeNet.blob\t       SampLeNet.prototxt\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/open_model_zoo/tools/accuracy_checker/data/test_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RpOyw9dZQ_1U"
   },
   "source": [
    "Here are SampleNets trained with different frameworks. Let's take Caffe one and convert it to IR.\n",
    "BKM: To achieve proper accuracy always check what normalization was applied on the model training stage. Model Optimizert can apply mean and scale values if appropriate. This information can be obtained from the model training script.\n",
    "In our case means and scales were applied to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SmKq9KKuQ_1V",
    "outputId": "02836829-9758-495c-8481-99923d46676c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/opt/intel/openvino/deployment_tools/open_model_zoo/tools/accuracy_checker/data/test_models/samplenet.onnx\n",
      "\t- Path for generated IR: \t/home/u40686/My-Notebooks/POT_training_simplenet/IR\n",
      "\t- IR output name: \tsamplenet\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \t[125.307,122.961,113.8575]\n",
      "\t- Scale values: \t[51.5865,50.847,51.255]\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "Model Optimizer version: \t2020.2.0-60-g0bc66e26ff\n",
      "\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/u40686/My-Notebooks/POT_training_simplenet/IR/samplenet.xml\n",
      "[ SUCCESS ] BIN file: /home/u40686/My-Notebooks/POT_training_simplenet/IR/samplenet.bin\n",
      "[ SUCCESS ] Total execution time: 1.31 seconds. \n",
      "[ SUCCESS ] Memory consumed: 74 MB. \n"
     ]
    }
   ],
   "source": [
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model /opt/intel/openvino/deployment_tools/open_model_zoo/tools/accuracy_checker/data/test_models/SampLeNet.caffemodel \\\n",
    "--output_dir IR \\\n",
    "--mean_values [125.307,122.961,113.8575] \\\n",
    "--scale_values [51.5865,50.847,51.255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6t8hsPLBQ_1Y"
   },
   "source": [
    "### Step 0.2. Getting the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAgbF_nyQ_1Y"
   },
   "source": [
    "The dataset is an essential part of quantization. It's needed to collect calibration statistics and measure accuracy using accuracy chcker tool. That's why big portion of information we'll need further is about the dataset on which model was trained\n",
    "Let's download download and prepare that. \n",
    "In our example the model was trained on cifar10 which consists of 60000 32x32 colour images in 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1VtTrsFQ_1Z",
    "outputId": "de0121dc-0fe1-4c1f-8c0b-41eb36f34e23"
   },
   "outputs": [],
   "source": [
    "!wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a3HdKsbuQ_1c"
   },
   "source": [
    "Unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6GobgK1_Q_1c"
   },
   "outputs": [],
   "source": [
    "!tar -xzf cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZ-xS57fQ_1f",
    "outputId": "e3be63af-a4b9-44b9-cc99-570be020b882"
   },
   "outputs": [],
   "source": [
    "!ls cifar-10-batches-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAhdRZGcQ_1j"
   },
   "source": [
    "## Step 1: Getting familiar with POT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iJVXu5DQ_1k"
   },
   "source": [
    "Lets check how POT is working and how to work with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0jlcpmRQ_1k",
    "outputId": "76409868-1375-4968-87be-98e2e84da50e"
   },
   "outputs": [],
   "source": [
    "!pot -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uO85CCALQ_1n"
   },
   "source": [
    "As you can see, all \"magic\" is inside the config.json file, lets look into that. \n",
    "OpenVINO has POT config templates and config examples inside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WgQP0GK_Q_1o",
    "outputId": "4cbfac5a-31ac-4782-d7ec-100dee139d18"
   },
   "outputs": [],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PHZFdidQ_1q"
   },
   "source": [
    "Templates (template_accuracy_aware_quantization.json, template_default_quantization.json, template_tpe.json) contain all possible POT parameters with very detailed explanation. If you need no know meaning of certain parameter, this is a goos resource to learn. \n",
    "Examples provide accuracy_checker, qunatization and sparsity configs for several well-known public topologies. This is good material to getting started if you need to quantize models listed here or similar models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GC4Eh28pQ_1s",
    "outputId": "2f4008e9-ba0c-4f66-9b8f-a08601205033"
   },
   "outputs": [],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "srEL246dQ_1u",
    "outputId": "b3790c6b-1b82-411f-cc4f-c8c15626eac9"
   },
   "outputs": [],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/examples/accuracy_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vO3mxrpRQ_1w",
    "outputId": "42398992-1e78-47de-c972-343f59f75c9b"
   },
   "outputs": [],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/examples/quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjykMtPpQ_1z",
    "outputId": "b7f0693f-a62d-447c-e12d-95287424e376"
   },
   "outputs": [],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/examples/quantization/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KaeOk-A8Q_11",
    "outputId": "8bfdc711-e3c8-45e8-eef4-35f17173b54a"
   },
   "outputs": [],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/examples/sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nP2jUg1bQ_13"
   },
   "source": [
    "Let's take a look into the DefaultQuantization template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kKFb4qVTQ_14",
    "outputId": "f66dd360-786f-4c9e-e72d-e3a727a62a2f"
   },
   "outputs": [],
   "source": [
    "!cat /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/template_default_quantization.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BWYdrOrqQ_16"
   },
   "source": [
    "It contains 3 main sections: \"model\", \"engine\", \"compression\".\n",
    "\n",
    "\"model\" is a simpliest, it keeps model name and path to IR.\n",
    "\n",
    "\"engine\" includes information of how the model will be executed. There are 2 modes: \"simplified\" - to run basic scenarios to roughly estimate performance gain and doesn't requre dataset labels; and \"accuracy checker\" - allows to produce more accurate qinatized model, allowes to tune the image preprocessing, reading, etc., labeled dataset required.\n",
    "\n",
    "\"compression\" section should have all needed optimization algorithm onformation. \n",
    "\n",
    "More detailed information is available at the README.md file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdmPR7dVQ_16",
    "outputId": "e336e88f-c265-4630-a2fe-8485a7eb2a12"
   },
   "outputs": [],
   "source": [
    "!cat /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/configs/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8iG47V-Q_19"
   },
   "source": [
    "Lets run POT in differenmt modes, compare them and prectice with POT configs creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLXh4lfWQ_1-"
   },
   "source": [
    "## Step 2. Rough INT8 performance estimation (simplified mode). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkymYgMkQ_1_"
   },
   "source": [
    "Let's imagine if we have the model and we're not satisfied with it's performance level. Low precision quantization is one of the optimization options. But not all models are well-qunatizable - sometimes the performance gain can be insignificant and use this approach is a waste of the time. It's really depends on the workload model/data etc.. You can quickly check whether it's worth to apply quantyzation technique or not using \"simplified mode\". Let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gs7r4r0wQ_1_"
   },
   "source": [
    "First of all, lets estimate the performance of the full pfecision (FP32) model using benchmark app. Benchmark app is a specual tool recommended for performance estimation. This is how to work with the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G3Vu06trQ_2A",
    "outputId": "cdc780b2-ee4d-4610-9922-35f31c08cf22"
   },
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xGfckD_Q_2C",
    "outputId": "010db970-e61b-465c-9c9d-bb0e008f15b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. \n",
      "[Step 2/11] Loading Inference Engine\n",
      "[ INFO ] InferenceEngine:\n",
      "         API version............. 2.1.42025\n",
      "[ INFO ] Device info\n",
      "         CPU\n",
      "         MKLDNNPlugin............ version 2.1\n",
      "         Build................... 42025\n",
      "\n",
      "[Step 3/11] Reading the Intermediate Representation network\n",
      "[ INFO ] Read network took 8.79 ms\n",
      "[Step 4/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Network batch size: 1\n",
      "[Step 5/11] Configuring input of the model\n",
      "[Step 6/11] Setting device configuration\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Load network took 46.60 ms\n",
      "[Step 8/11] Setting optimal runtime parameters\n",
      "[Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[ INFO ] Network input 'data' precision U8, dimensions (NCHW): 1 3 32 32\n",
      "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[ INFO ] Infer Request 0 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 1 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 2 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 3 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 4 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 5 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 6 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 7 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 8 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 9 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 10 inference requests using 10 streams for CPU, limits: 60000 ms duration)\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:      4534740 iterations\n",
      "Duration:   60000.19 ms\n",
      "Latency:    0.12 ms\n",
      "Throughput: 75578.76 FPS\n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py -m IR/SampLeNet.xml -i cifar-10-python/01_cat.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zK1enU25Q_2E"
   },
   "source": [
    "That's great throughput and latency numbers, but lets see can we improve it or not. So, we need to create POT config for that purpose. To use simplified mode we need specify \"type\": \"simplified\" and \"data_source\": \"path/to/the/dataset\" fields at the compression section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eDcV_58Q_2F",
    "outputId": "a8b88919-f948-475e-9188-66068b9dfa54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": {\n",
      "    \"model_name\": \"SampLeNet_simplified\",\n",
      "    \"model\": \"IR/SampLeNet.xml\",\n",
      "    \"weights\": \"IR/SampLeNet.bin\"\n",
      "  },\n",
      "  \"engine\": {\n",
      "    \"type\": \"simplified\",\n",
      "    \"data_source\": \"cifar-10-python\" \n",
      "  },\n",
      "  \"compression\": {\n",
      "    \"target_device\": \"CPU\",\n",
      "    \"algorithms\": [\n",
      "      {\n",
      "        \"name\": \"DefaultQuantization\",\n",
      "        \"params\": {\n",
      "          \"preset\": \"performance\",\n",
      "          \"stat_subset_size\": 300,\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat SampLeNet_simplified.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run quantization in simplified mode. Using \"-d\" option to simplify results stirage and further reuse of the models by benchmark app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z65mUAmhQ_2G",
    "outputId": "4e9fdf70-50c9-4b3e-d530-dd7a58cdcde9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:app.run:Output log dir: ./results\n",
      "INFO:app.run:Creating pipeline:\n",
      " Algorithm: DefaultQuantization\n",
      " Parameters:\n",
      "\tpreset                     : performance\n",
      "\tstat_subset_size           : 300\n",
      "\ttarget_device              : CPU\n",
      "\texec_log_dir               : ./results\n",
      " ===========================================================================\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : DefaultQuantization\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.pipeline.pipeline:Start algorithm: DefaultQuantization\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : ActivationChannelAlignment\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : MinMaxQuantization,FastBiasCorrection\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.pipeline.pipeline:Finished: DefaultQuantization\n",
      " ===========================================================================\n"
     ]
    }
   ],
   "source": [
    "!pot -c SampLeNet_simplified.json -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6SPfe_ZQ_2I"
   },
   "source": [
    "Rough estimation of how we can benefit from INT8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HW2SSfCpQ_2J",
    "outputId": "15acaf46-1475-4632-86f4-2f3bbbd80d70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. \n",
      "[Step 2/11] Loading Inference Engine\n",
      "[ INFO ] InferenceEngine:\n",
      "         API version............. 2.1.42025\n",
      "[ INFO ] Device info\n",
      "         CPU\n",
      "         MKLDNNPlugin............ version 2.1\n",
      "         Build................... 42025\n",
      "\n",
      "[Step 3/11] Reading the Intermediate Representation network\n",
      "[ INFO ] Read network took 14.69 ms\n",
      "[Step 4/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Network batch size: 1\n",
      "[Step 5/11] Configuring input of the model\n",
      "[Step 6/11] Setting device configuration\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Load network took 91.30 ms\n",
      "[Step 8/11] Setting optimal runtime parameters\n",
      "[Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[ INFO ] Network input 'data' precision U8, dimensions (NCHW): 1 3 32 32\n",
      "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[ INFO ] Infer Request 0 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 1 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 2 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 3 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 4 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 5 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 6 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 7 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 8 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 9 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 10 inference requests using 10 streams for CPU, limits: 60000 ms duration)\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:      5596320 iterations\n",
      "Duration:   60000.22 ms\n",
      "Latency:    0.09 ms\n",
      "Throughput: 93271.65 FPS\n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py -m results/optimized/SampLeNet_simplified.xml -i cifar-10-python/01_cat.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Measuring accuracy of FP32 model. Accuracy Checker configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bf8yQMu7Q_2M"
   },
   "source": [
    "Looks like we can dramatically accelerate our workload. But what's about performance. If we're able to run the model with benchmark it's not guaranteed that the inference result is correct. To confirm that the model output is correct you can check visually/manually, but we're offering Accuracy Checker tool which estimates accuracy metrics of given model on given dataset. It can be used directly using accuracy_check alias or call it from the POT config. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kL6p_E4Q_2M",
    "outputId": "69a91824-3743-4d12-a8e5-15f7e1ac6627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:55:51 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\n",
      "06:55:51 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\n",
      "06:55:51 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\n",
      "06:55:51 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\n",
      "06:55:51 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\n",
      "06:55:51 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\n",
      "06:55:53 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\n",
      "06:55:53 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\n",
      "06:55:53 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\n",
      "06:55:53 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\n",
      "06:55:53 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\n",
      "06:55:53 accuracy_checker WARNING: /usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\n",
      "usage: accuracy_check [-h] [-d DEFINITIONS] -c CONFIG [-m MODELS [MODELS ...]]\n",
      "                      [-s SOURCE] [-a ANNOTATIONS] [-e EXTENSIONS]\n",
      "                      [--cpu_extensions_mode {avx512,avx2,sse4}]\n",
      "                      [-b BITSTREAMS]\n",
      "                      [--stored_predictions STORED_PREDICTIONS]\n",
      "                      [-C CONVERTED_MODELS] [-M MODEL_OPTIMIZER]\n",
      "                      [--tf_custom_op_config_dir TF_CUSTOM_OP_CONFIG_DIR]\n",
      "                      [--transformations_config_dir TRANSFORMATIONS_CONFIG_DIR]\n",
      "                      [--tf_obj_detection_api_pipeline_config_path TF_OBJ_DETECTION_API_PIPELINE_CONFIG_PATH]\n",
      "                      [--progress PROGRESS]\n",
      "                      [--progress_interval PROGRESS_INTERVAL]\n",
      "                      [-tf TARGET_FRAMEWORK]\n",
      "                      [-td TARGET_DEVICES [TARGET_DEVICES ...]]\n",
      "                      [-tt TARGET_TAGS [TARGET_TAGS ...]] [-l LOG_FILE]\n",
      "                      [--ignore_result_formatting IGNORE_RESULT_FORMATTING]\n",
      "                      [-am AFFINITY_MAP] [--aocl AOCL]\n",
      "                      [--vpu_log_level {LOG_NONE,LOG_WARNING,LOG_INFO,LOG_DEBUG}]\n",
      "                      [--deprecated_ir_v7 DEPRECATED_IR_V7]\n",
      "                      [-dc DEVICE_CONFIG] [--async_mode ASYNC_MODE]\n",
      "                      [--num_requests NUM_REQUESTS] [--csv_result CSV_RESULT]\n",
      "                      [--model_is_blob MODEL_IS_BLOB]\n",
      "\n",
      "Deep Learning accuracy validation framework\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -d DEFINITIONS, --definitions DEFINITIONS\n",
      "                        path to the yml file with definitions\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        path to the yml file with local configuration\n",
      "  -m MODELS [MODELS ...], --models MODELS [MODELS ...]\n",
      "                        prefix path to the models and weights\n",
      "  -s SOURCE, --source SOURCE\n",
      "                        prefix path to the data source\n",
      "  -a ANNOTATIONS, --annotations ANNOTATIONS\n",
      "                        prefix path to the converted annotations and datasets\n",
      "                        meta data\n",
      "  -e EXTENSIONS, --extensions EXTENSIONS\n",
      "                        prefix path to extensions folder\n",
      "  --cpu_extensions_mode {avx512,avx2,sse4}\n",
      "                        specified preferable set of processor instruction for\n",
      "                        automatic searching cpu extension lib\n",
      "  -b BITSTREAMS, --bitstreams BITSTREAMS\n",
      "                        prefix path to bitstreams folder\n",
      "  --stored_predictions STORED_PREDICTIONS\n",
      "                        path to file with saved predictions. Used for\n",
      "                        development\n",
      "  -C CONVERTED_MODELS, --converted_models CONVERTED_MODELS\n",
      "                        directory to store Model Optimizer converted models.\n",
      "                        Used for DLSDK launcher only\n",
      "  -M MODEL_OPTIMIZER, --model_optimizer MODEL_OPTIMIZER\n",
      "                        path to model optimizer directory\n",
      "  --tf_custom_op_config_dir TF_CUSTOM_OP_CONFIG_DIR\n",
      "                        path to directory with tensorflow custom operation\n",
      "                        configuration files for model optimizer\n",
      "  --transformations_config_dir TRANSFORMATIONS_CONFIG_DIR\n",
      "                        path to directory with Model Optimizer transformations\n",
      "                        configuration files\n",
      "  --tf_obj_detection_api_pipeline_config_path TF_OBJ_DETECTION_API_PIPELINE_CONFIG_PATH\n",
      "                        path to directory with tensorflow object detection api\n",
      "                        pipeline configuration files for model optimizer\n",
      "  --progress PROGRESS   progress reporter. You can select bar or print\n",
      "  --progress_interval PROGRESS_INTERVAL\n",
      "                        interval for update progress if selected *print*\n",
      "                        progress.\n",
      "  -tf TARGET_FRAMEWORK, --target_framework TARGET_FRAMEWORK\n",
      "                        framework for infer\n",
      "  -td TARGET_DEVICES [TARGET_DEVICES ...], --target_devices TARGET_DEVICES [TARGET_DEVICES ...]\n",
      "                        Space separated list of devices for infer\n",
      "  -tt TARGET_TAGS [TARGET_TAGS ...], --target_tags TARGET_TAGS [TARGET_TAGS ...]\n",
      "                        Space separated list of launcher tags for infer\n",
      "  -l LOG_FILE, --log_file LOG_FILE\n",
      "                        file for additional logging results\n",
      "  --ignore_result_formatting IGNORE_RESULT_FORMATTING\n",
      "                        allow to get raw metrics results without data\n",
      "                        formatting\n",
      "  -am AFFINITY_MAP, --affinity_map AFFINITY_MAP\n",
      "                        prefix path to the affinity maps\n",
      "  --aocl AOCL           path to aocl executable for FPGA bitstream programming\n",
      "  --vpu_log_level {LOG_NONE,LOG_WARNING,LOG_INFO,LOG_DEBUG}\n",
      "                        log level for VPU devices\n",
      "  --deprecated_ir_v7 DEPRECATED_IR_V7\n",
      "                        Allow generation IR v7 via Model Optimizer\n",
      "  -dc DEVICE_CONFIG, --device_config DEVICE_CONFIG\n",
      "                        Inference Engine device specific config file\n",
      "  --async_mode ASYNC_MODE\n",
      "                        Allow evaluation in async mode\n",
      "  --num_requests NUM_REQUESTS\n",
      "                        the number of infer requests\n",
      "  --csv_result CSV_RESULT\n",
      "                        file for results writing\n",
      "  --model_is_blob MODEL_IS_BLOB\n",
      "                        the tip for automatic model search to use blob for\n",
      "                        dlsdk launcher\n"
     ]
    }
   ],
   "source": [
    "!accuracy_check -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xN_sav7eQ_2O"
   },
   "source": [
    "It was how to use Accuracy Checker directly, it uses it's own .yaml configuration files. But to avoid producing too many configs, here we'll call it from \"engine\" section at the same POT config. Lets change it like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Vm_G2yHQ_2O",
    "outputId": "537c6df1-1c73-44a0-c963-b64be3417c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": {\n",
      "    \"model_name\": \"SampLeNet_FP32\",\n",
      "    \"model\": \"IR/SampLeNet.xml\",\n",
      "    \"weights\": \"IR/SampLeNet.bin\"\n",
      "  },\n",
      "  \"engine\": {\n",
      "    \"launchers\": [\n",
      "      {\n",
      "        \"framework\": \"dlsdk\",\n",
      "        \"device\": \"CPU\",\n",
      "        \"adapter\": \"classification\"\n",
      "      }\n",
      "    ],\n",
      "    \"datasets\": [\n",
      "      {\n",
      "        \"name\": \"classification_dataset\",\n",
      "        \"data_source\": \"cifar-10-python\",\n",
      "        \"annotation_conversion\": {\n",
      "          \"converter\": \"cifar\",\n",
      "          \"data_batch_file\": \"cifar-10-batches-py/test_batch\",\n",
      "          \"convert_images\": true,\n",
      "          \"converted_images_dir\": \"cifar-10-python\",\n",
      "          \"num_classes\": 10\n",
      "        },\n",
      "        \"reader\": \"pillow_imread\",\n",
      "        \"metrics\": [\n",
      "          {\n",
      "            \"name\": \"accuracy@top1\",\n",
      "            \"type\": \"accuracy\",\n",
      "            \"top_k\": 1\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"accuracy@top5\",\n",
      "            \"type\": \"accuracy\",\n",
      "            \"top_k\": 5\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat SampLeNet_FP32.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dyklDaCQ_2R"
   },
   "source": [
    "**Accuracy Checker configuration files have the following parts:**\n",
    "\n",
    "* **Launchers** are the inference backend. It can be OpenVINO Inference Engine or fameworks like Tensorflow, Pytorch, ONNX Runtime, etc. So, choosong different launchers you can compare an accuracy of the model inferred using OpenVINO and by the framemowork it was trained. Here you can also specify inference device. At the example above we're using Inference Engine Launcher on CPU. Click [here](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_README.html) to see full launchers list.\n",
    "\n",
    "   **Please note that for quanization via POT OpenVINO Inference Engine is  only available inference backend.**\n",
    "\n",
    "* **Adapters**. Adapter converts network infer output to metric specific format. [Here are available adapters](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_accuracy_checker_adapters_README.html). We have simple classification model, so, our adapter is \"classification\". \n",
    "\n",
    "* **Annotation Converters**. Today there are thousants of datasets and eche has its own annotation format. AC uses it's own internal dataset annotation format. So, annotation converted convers the dataset annotation from its fromat to AC one. [Check supported datasets](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_accuracy_checker_annotation_converters_README.html)\n",
    "    At this example we're using cifar10 - lets specify that, and don't forget put the numper of classes.\n",
    "\n",
    "* **Readers**. It's how images from the dataset will be read. Look for [implemented readerd](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_accuracy_checker_data_readers_README.html)\n",
    "We're going to read RGB images in .png format - several readers from the list works for us. OpenCV imread is the default one, we'll use because the same reader was used on the training stage - so, that makes our experimets more precise.\n",
    "\n",
    "* [Preprocesors](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_accuracy_checker_preprocessor_README.html) and [Postprocessors](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_accuracy_checker_postprocessor_README.html) - addotional calibration datased pre- and post-processing can be added if appropriate (it that's done in model training). The most popular preprocessing is resize images to fit model input shape. In our case it is not required because images already have the same size like model input. We alredy added some normalization (scale and mean values) on Model Optimizer side, so, we don't need to put it here. If we forget to add \"--mean\" and -\"--scale\" at the MO command line, we can do it here. Only one preprocessing we should add is BGR to RGB conversion, because topology is trained on RGB images, but OpenCV reader (opencv_imread) reads in BGR.\n",
    "\n",
    "* **Metrics**. It's the way of accuracy measuring. Different CV tasks like classification, detection, segmentation, etc has diferent approaches to measure accuracy. Ypu can choose among [these metrics](https://docs.openvinotoolkit.org/latest/_tools_accuracy_checker_accuracy_checker_metrics_README.html).\n",
    "Our SampLeNet is a classification model, cifar is classification dataset, so, lets apply most popular classification metrics: `top1` and `top5`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7FyrBMKQ_2S"
   },
   "source": [
    "To calculate accuracy of full precision model using POT and Accuracy Checker - leave empty \"compression\" section at the POT config and add \"-e\" (evaluation) parameter to the command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FS6UQnmwQ_2S",
    "outputId": "8c218bec-bb98-4d97-a901-20059eb82df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:app.run:Output log dir: ./results\n",
      "INFO:app.run:Creating pipeline:\n",
      " ===========================================================================\n",
      "IE version: 2.1.42025\n",
      "Loaded CPU plugin version:\n",
      "    CPU - MKLDNNPlugin: 2.1.42025\n",
      "INFO:compression.pipeline.pipeline:Evaluation of generated model\n",
      "INFO:compression.engines.ac_engine:Start inference on the whole dataset\n",
      "Total dataset size: 10000\n",
      "1000 / 10000 processed in 2.036s\n",
      "2000 / 10000 processed in 1.999s\n",
      "3000 / 10000 processed in 2.038s\n",
      "4000 / 10000 processed in 2.005s\n",
      "5000 / 10000 processed in 2.041s\n",
      "6000 / 10000 processed in 2.001s\n",
      "7000 / 10000 processed in 2.037s\n",
      "8000 / 10000 processed in 1.999s\n",
      "9000 / 10000 processed in 2.030s\n",
      "10000 / 10000 processed in 2.016s\n",
      "10000 objects processed in 20.202 seconds\n",
      "INFO:compression.engines.ac_engine:Inference finished\n",
      "INFO:app.run:accuracy@top1              : 0.7502\n",
      "INFO:app.run:accuracy@top5              : 0.9822\n"
     ]
    }
   ],
   "source": [
    "!pot -c SampLeNet_FP32.json -e -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlKcGyCFQ_2U"
   },
   "source": [
    "FP32 model accuracy level is exactly the same with Caffe output, so, our model is running correctly. Lets quantize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Running calibration algorithms. Evaluating it's accuracy and perfromance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0sKl8RWQ_2U"
   },
   "source": [
    "Currently (OpenVINO 2020.2) there are 2 \"production quality\" quantization algorithms: DefaultQauntization and AccuracyAwareQuantization. Lets read how do they work at the README file below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJ9F_NuAQ_2V",
    "outputId": "75d4348b-921d-4981-8431-3fb9eee86a43"
   },
   "outputs": [],
   "source": [
    "!cat /opt/intel/openvino/deployment_tools/tools/post_training_optimization_toolkit/compression/algorithms/quantization/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1 Running DefaultQuantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ElFg5QXTQ_2W"
   },
   "source": [
    "Let's apply DefaultQuantization, filing \"compression\" section as at the config below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "acmycRKPQ_2X",
    "outputId": "ce261835-0a38-4676-a579-12fa2f71c4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": {\n",
      "    \"model_name\": \"SampLeNet_DefaultQuantization\",\n",
      "    \"model\": \"IR/SampLeNet.xml\",\n",
      "    \"weights\": \"IR/SampLeNet.bin\"\n",
      "  },\n",
      "  \"engine\": {\n",
      "    \"launchers\": [\n",
      "      {\n",
      "        \"framework\": \"dlsdk\",\n",
      "        \"device\": \"CPU\",\n",
      "        \"adapter\": \"classification\"\n",
      "      }\n",
      "    ],\n",
      "    \"datasets\": [\n",
      "      {\n",
      "        \"name\": \"classification_dataset\",\n",
      "        \"data_source\": \"cifar-10-python\",\n",
      "        \"annotation_conversion\": {\n",
      "          \"converter\": \"cifar\",\n",
      "          \"data_batch_file\": \"cifar-10-batches-py/test_batch\",\n",
      "          \"convert_images\": true,\n",
      "          \"converted_images_dir\": \"cifar-10-python\",\n",
      "          \"num_classes\": 10\n",
      "        },\n",
      "        \"reader\": \"opencv_imread\",\n",
      "        \"preprocessing\": [\n",
      "            {\n",
      "                \"type\": \"bgr_to_rgb\"\n",
      "            }\n",
      "        ],\n",
      "        \"metrics\": [\n",
      "          {\n",
      "            \"name\": \"accuracy@top1\",\n",
      "            \"type\": \"accuracy\",\n",
      "            \"top_k\": 1\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"accuracy@top5\",\n",
      "            \"type\": \"accuracy\",\n",
      "            \"top_k\": 5\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "    \"compression\": {\n",
      "    \"target_device\": \"CPU\",\n",
      "    \"algorithms\": [\n",
      "      {\n",
      "        \"name\": \"DefaultQuantization\",\n",
      "        \"params\": {\n",
      "          \"preset\": \"performance\",\n",
      "          \"stat_subset_size\": 300\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat SampLeNet_DefaultQuantization.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCNntEU5Q_2Y"
   },
   "source": [
    "Running it, again using \"-e\" option to see accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_SHVc-fMQ_2Z",
    "outputId": "7a7dd9a0-abf0-406a-c833-db69e6b522d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:app.run:Output log dir: ./results\n",
      "INFO:app.run:Creating pipeline:\n",
      " Algorithm: DefaultQuantization\n",
      " Parameters:\n",
      "\tpreset                     : performance\n",
      "\tstat_subset_size           : 300\n",
      "\ttarget_device              : CPU\n",
      "\texec_log_dir               : ./results\n",
      " ===========================================================================\n",
      "IE version: 2.1.42025\n",
      "Loaded CPU plugin version:\n",
      "    CPU - MKLDNNPlugin: 2.1.42025\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : DefaultQuantization\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.pipeline.pipeline:Start algorithm: DefaultQuantization\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : ActivationChannelAlignment\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : MinMaxQuantization,FastBiasCorrection\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.pipeline.pipeline:Finished: DefaultQuantization\n",
      " ===========================================================================\n",
      "INFO:compression.pipeline.pipeline:Evaluation of generated model\n",
      "INFO:compression.engines.ac_engine:Start inference on the whole dataset\n",
      "Total dataset size: 10000\n",
      "1000 / 10000 processed in 1.999s\n",
      "2000 / 10000 processed in 2.107s\n",
      "3000 / 10000 processed in 2.153s\n",
      "4000 / 10000 processed in 2.137s\n",
      "5000 / 10000 processed in 2.125s\n",
      "6000 / 10000 processed in 2.153s\n",
      "7000 / 10000 processed in 2.121s\n",
      "8000 / 10000 processed in 2.148s\n",
      "9000 / 10000 processed in 2.153s\n",
      "10000 / 10000 processed in 2.138s\n",
      "10000 objects processed in 21.235 seconds\n",
      "INFO:compression.engines.ac_engine:Inference finished\n",
      "INFO:app.run:accuracy@top1              : 0.7414\n",
      "INFO:app.run:accuracy@top5              : 0.9809\n"
     ]
    }
   ],
   "source": [
    "!pot -c SampLeNet_DefaultQuantization.json -e -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PJN1rHYQ_2b"
   },
   "source": [
    "Accuracy doesn't deviate so much from FP32 model - we can stop here, but lets' check AccuracyAwareAlgorithm. Just change the algo name in compression section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2 Running AccuracyAwareQuantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HSoEMFQQ_2c",
    "outputId": "9ff07aa8-1ff1-40cb-e948-4f0658e5a461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": {\n",
      "    \"model_name\": \"SampLeNet_AccuracyAware\",\n",
      "    \"model\": \"IR/SampLeNet.xml\",\n",
      "    \"weights\": \"IR/SampLeNet.bin\"\n",
      "  },\n",
      "  \"engine\": {\n",
      "    \"launchers\": [\n",
      "      {\n",
      "        \"framework\": \"dlsdk\",\n",
      "        \"device\": \"CPU\",\n",
      "        \"adapter\": \"classification\"\n",
      "      }\n",
      "    ],\n",
      "    \"datasets\": [\n",
      "      {\n",
      "        \"name\": \"classification_dataset\",\n",
      "        \"data_source\": \"cifar-10-python\",\n",
      "        \"annotation_conversion\": {\n",
      "          \"converter\": \"cifar\",\n",
      "          \"data_batch_file\": \"cifar-10-batches-py/test_batch\",\n",
      "          \"convert_images\": true,\n",
      "          \"converted_images_dir\": \"cifar-10-python\",\n",
      "          \"num_classes\": 10\n",
      "        },\n",
      "        \"reader\": \"pillow_imread\",\n",
      "        \"metrics\": [\n",
      "          {\n",
      "            \"name\": \"accuracy@top1\",\n",
      "            \"type\": \"accuracy\",\n",
      "            \"top_k\": 1\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"accuracy@top5\",\n",
      "            \"type\": \"accuracy\",\n",
      "            \"top_k\": 5\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"compression\": {\n",
      "    \"target_device\": \"CPU\",\n",
      "    \"algorithms\": [\n",
      "      {\n",
      "        \"name\": \"AccuracyAwareQuantization\",\n",
      "        \"params\": {\n",
      "          \"preset\": \"performance\",\n",
      "          \"stat_subset_size\": 300\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat SampLeNet_AccuracyAwareQuantization.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGheA0sZQ_2e",
    "outputId": "9e3d4f98-e49d-470a-c4e1-208553ae767d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:app.run:Output log dir: ./results\n",
      "INFO:app.run:Creating pipeline:\n",
      " Algorithm: AccuracyAwareQuantization\n",
      " Parameters:\n",
      "\tpreset                     : performance\n",
      "\tstat_subset_size           : 300\n",
      "\ttarget_device              : CPU\n",
      "\texec_log_dir               : ./results\n",
      " ===========================================================================\n",
      "IE version: 2.1.42025\n",
      "Loaded CPU plugin version:\n",
      "    CPU - MKLDNNPlugin: 2.1.42025\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : AccuracyAwareQuantization\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.pipeline.pipeline:Start algorithm: AccuracyAwareQuantization\n",
      "INFO:compression.algorithms.quantization.accuracy_aware.algorithm:Start original model inference\n",
      "INFO:compression.engines.ac_engine:Start inference of 10000 images\n",
      "Total dataset size: 10000\n",
      "1000 / 10000 processed in 1.545s\n",
      "2000 / 10000 processed in 1.504s\n",
      "3000 / 10000 processed in 1.560s\n",
      "4000 / 10000 processed in 1.492s\n",
      "5000 / 10000 processed in 1.519s\n",
      "6000 / 10000 processed in 1.473s\n",
      "7000 / 10000 processed in 1.517s\n",
      "8000 / 10000 processed in 1.501s\n",
      "9000 / 10000 processed in 1.527s\n",
      "10000 / 10000 processed in 1.504s\n",
      "10000 objects processed in 15.140 seconds\n",
      "INFO:compression.engines.ac_engine:Inference finished\n",
      "INFO:compression.algorithms.quantization.accuracy_aware.algorithm:Baseline metrics: {'accuracy@top1': 0.7502, 'accuracy@top5': 0.9822}\n",
      "INFO:compression.algorithms.quantization.accuracy_aware.algorithm:Start quantization\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : ActivationChannelAlignment\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.statistics.collector:Start computing statistics for algorithms : MinMaxQuantization,FastBiasCorrection\n",
      "INFO:compression.statistics.collector:Computing statistics finished\n",
      "INFO:compression.algorithms.quantization.accuracy_aware.algorithm:Start compressed model inference\n",
      "INFO:compression.engines.ac_engine:Start inference of 10000 images\n",
      "Total dataset size: 10000\n",
      "1000 / 10000 processed in 1.759s\n",
      "2000 / 10000 processed in 1.803s\n",
      "3000 / 10000 processed in 1.838s\n",
      "4000 / 10000 processed in 1.833s\n",
      "5000 / 10000 processed in 1.827s\n",
      "6000 / 10000 processed in 1.814s\n",
      "7000 / 10000 processed in 1.836s\n",
      "8000 / 10000 processed in 1.810s\n",
      "9000 / 10000 processed in 1.846s\n",
      "10000 / 10000 processed in 1.804s\n",
      "10000 objects processed in 18.171 seconds\n",
      "INFO:compression.engines.ac_engine:Inference finished\n",
      "INFO:compression.algorithms.quantization.accuracy_aware.algorithm:Fully quantized metrics: {'accuracy@top1': 0.7414, 'accuracy@top5': 0.9809}\n",
      "INFO:compression.algorithms.quantization.accuracy_aware.algorithm:Accuracy drop: {'accuracy@top1': 0.00880000000000003, 'accuracy@top5': 0.0012999999999999678}\n",
      "INFO:compression.pipeline.pipeline:Finished: AccuracyAwareQuantization\n",
      " ===========================================================================\n",
      "INFO:compression.pipeline.pipeline:Evaluation of generated model\n",
      "INFO:compression.engines.ac_engine:Start inference on the whole dataset\n",
      "Total dataset size: 10000\n",
      "1000 / 10000 processed in 1.691s\n",
      "2000 / 10000 processed in 1.765s\n",
      "3000 / 10000 processed in 1.797s\n",
      "4000 / 10000 processed in 1.773s\n",
      "5000 / 10000 processed in 1.773s\n",
      "6000 / 10000 processed in 1.802s\n",
      "7000 / 10000 processed in 1.768s\n",
      "8000 / 10000 processed in 1.758s\n",
      "9000 / 10000 processed in 1.818s\n",
      "10000 / 10000 processed in 1.763s\n",
      "10000 objects processed in 17.707 seconds\n",
      "INFO:compression.engines.ac_engine:Inference finished\n",
      "INFO:app.run:accuracy@top1              : 0.7414\n",
      "INFO:app.run:accuracy@top5              : 0.9809\n"
     ]
    }
   ],
   "source": [
    "!pot -c SampLeNet_AccuracyAwareQuantization.json -e -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJ0fkelqQ_2m"
   },
   "source": [
    "AccuracyAwareQuantization produced INT8 model with the same accuracy level with default one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKZL7yUwQ_2m"
   },
   "source": [
    "Now it's time to compare performance level of quantized model using Benchmark App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJXIFMeMQ_2n",
    "outputId": "4703a293-7963-4825-9d41-5c421a56ba25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. \n",
      "[Step 2/11] Loading Inference Engine\n",
      "[ INFO ] InferenceEngine:\n",
      "         API version............. 2.1.42025\n",
      "[ INFO ] Device info\n",
      "         CPU\n",
      "         MKLDNNPlugin............ version 2.1\n",
      "         Build................... 42025\n",
      "\n",
      "[Step 3/11] Reading the Intermediate Representation network\n",
      "[ INFO ] Read network took 14.91 ms\n",
      "[Step 4/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Network batch size: 1\n",
      "[Step 5/11] Configuring input of the model\n",
      "[Step 6/11] Setting device configuration\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Load network took 89.70 ms\n",
      "[Step 8/11] Setting optimal runtime parameters\n",
      "[Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[ INFO ] Network input 'data' precision U8, dimensions (NCHW): 1 3 32 32\n",
      "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[ INFO ] Infer Request 0 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 1 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 2 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 3 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 4 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 5 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 6 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 7 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 8 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[ INFO ] Infer Request 9 filling\n",
      "[ INFO ] Fill input 'data' with random values (image is expected)\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 10 inference requests using 10 streams for CPU, limits: 60000 ms duration)\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:      5561510 iterations\n",
      "Duration:   60000.14 ms\n",
      "Latency:    0.09 ms\n",
      "Throughput: 92691.61 FPS\n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py -m results/optimized/SampLeNet_DefaultQuantization.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNORg4wyQ_2q"
   },
   "source": [
    "Looks like we have good performance gain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpdesTpcQ_2q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "POT_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
